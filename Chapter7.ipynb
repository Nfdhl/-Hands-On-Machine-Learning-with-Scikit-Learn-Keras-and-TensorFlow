{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Rangkuman Chapter 7\n",
        "\n",
        "Chapter ini membahas **Ensemble Learning**, sebuah teknik yang menggabungkan prediksi dari sekelompok prediktor (disebut *ensemble*) untuk mendapatkan prediksi yang lebih baik daripada prediktor individu terbaik.\n",
        "\n",
        "Ide dasarnya mirip dengan **\"wisdom of the crowd\"**, di mana jawaban gabungan dari ribuan orang seringkali lebih baik daripada jawaban seorang ahli. Salah satu contoh paling terkenal adalah **Random Forest**, yang merupakan ensemble dari Decision Trees. Ensemble Learning sering digunakan untuk memenangkan kompetisi Machine Learning. Chapter ini akan mencakup metode ensemble populer: **bagging**, **boosting**, dan **stacking**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Topics Covered\n",
        "\n",
        "| No | Topic | Description |\n",
        "|----|-------|-------------|\n",
        "| 1 | **Voting Classifiers** | Menggabungkan prediksi dengan suara mayoritas (hard & soft voting) |\n",
        "| 2 | **Bagging & Pasting** | Melatih prediktor pada subset acak data (dengan/tanpa replacement) |\n",
        "| 3 | **Out-of-Bag (oob) Evaluation** | Mengevaluasi model bagging tanpa perlu validation set |\n",
        "| 4 | **Random Forests** | Ensemble Decision Trees yang dilatih dengan metode bagging |\n",
        "| 5 | **Extra-Trees** | Varian Random Forest yang lebih \"acak\" |\n",
        "| 6 | **Feature Importance** | Cara Random Forest mengukur pentingnya setiap fitur |\n",
        "| 7 | **Boosting (AdaBoost)** | Melatih prediktor secara berurutan, fokus pada kesalahan sebelumnya |\n",
        "| 8 | **Gradient Boosting** | Melatih prediktor secara berurutan, fokus pada *residual errors* |\n",
        "| 9 | **Stacking** | Menggunakan model (blender) untuk menggabungkan prediksi ensemble |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó≥Ô∏è Voting Classifiers\n",
        "\n",
        "Ini adalah cara sederhana untuk membuat classifier yang lebih baik: latih beberapa classifier yang berbeda (misalnya, Logistic Regression, SVM, Random Forest), lalu gabungkan prediksi mereka.\n",
        "\n",
        "* **Hard Voting Classifier**: Memprediksi kelas yang mendapatkan suara mayoritas (paling banyak dipilih) dari semua classifier. Anehnya, metode ini seringkali mencapai akurasi lebih tinggi daripada classifier terbaik di dalam ensemble.\n",
        "* **Soft Voting Classifier**: Jika semua classifier dapat menghitung probabilitas (`predict_proba()`), Anda dapat memprediksi kelas dengan probabilitas rata-rata tertinggi. Ini seringkali berkinerja lebih baik karena memberi bobot lebih pada \"suara\" yang sangat percaya diri.\n",
        "\n",
        "**Mengapa ini berhasil? (Law of Large Numbers)**\n",
        "Anggap Anda memiliki 1.000 classifier \"lemah\" (weak learners) yang hanya 51% akurat (sedikit lebih baik dari tebakan acak). Jika Anda mengambil suara mayoritas, akurasi ensemble bisa mencapai 75%!\n",
        "\n",
        "Ini berhasil dengan asumsi bahwa semua classifier **independen** dan membuat **kesalahan yang tidak berkorelasi**. Karena itu, metode ensemble bekerja paling baik ketika prediktornya **beragam (diverse)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load example dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rnd_clf = RandomForestClassifier(random_state=42)\n",
        "svm_clf = SVC(random_state=42)\n",
        "\n",
        "# Hard Voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõçÔ∏è Bagging and Pasting\n",
        "\n",
        "Ini adalah pendekatan lain untuk mendapatkan classifier yang beragam. Alih-alih menggunakan algoritme yang berbeda, Anda menggunakan **algoritme yang sama** (misalnya, Decision Tree) tetapi melatihnya pada **subset acak yang berbeda** dari training set.\n",
        "\n",
        "* **Bagging (Bootstrap Aggregating)**: Sampling dilakukan **dengan replacement** (`bootstrap=True`). Ini berarti satu instance bisa diambil beberapa kali untuk satu prediktor.\n",
        "* **Pasting**: Sampling dilakukan **tanpa replacement** (`bootstrap=False`).\n",
        "\n",
        "**Cara Kerja:**\n",
        "Setelah semua prediktor dilatih (bisa secara paralel), ensemble membuat prediksi dengan mengambil **mode** (untuk klasifikasi) atau **rata-rata** (untuk regresi) dari semua prediksi individu.\n",
        "\n",
        "**Hasil:** Ensemble memiliki **bias yang serupa** tetapi **variance yang lebih rendah** daripada satu prediktor yang dilatih pada semua data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Ensemble Bagging dengan 500 Decision Trees\n",
        "# Setiap pohon dilatih pada 100 instance acak (dengan replacement)\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Out-of-Bag (oob) Evaluation\n",
        "\n",
        "Saat menggunakan bagging (sampling dengan replacement), rata-rata hanya **63%** instance pelatihan yang di-sampling untuk setiap prediktor. Instance yang tersisa (tidak terpilih) disebut **Out-of-Bag (oob)**.\n",
        "\n",
        "Anda dapat mengevaluasi ensemble dengan menggunakan instance oob ini sebagai \"validation set gratis\" (`oob_score=True`). Ini memberikan estimasi performa model tanpa perlu validation set terpisah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bag_clf_oob = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, n_jobs=-1, oob_score=True\n",
        ")\n",
        "bag_clf_oob.fit(X_train, y_train)\n",
        "print(f\"OOB Score: {bag_clf_oob.oob_score_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üå≥ Random Patches & Random Subspaces\n",
        "\n",
        "Bagging juga bisa mengacak fitur (bukan hanya instance).\n",
        "\n",
        "* **Random Patches**: Sampling acak pada instance DAN fitur (`bootstrap=True`, `max_features < 1.0`, `bootstrap_features=True`).\n",
        "* **Random Subspaces**: Sampling acak pada fitur saja (`bootstrap=False`, `max_samples=1.0`, `bootstrap_features=True`, `max_features < 1.0`).\n",
        "\n",
        "Ini menambahkan lebih banyak keragaman (diversity), menukar bias yang sedikit lebih tinggi dengan variance lebih rendah. Berguna jika data memiliki banyak fitur (misalnya, gambar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üå≤ Random Forests\n",
        "\n",
        "Random Forest adalah ensemble dari Decision Trees, yang dilatih menggunakan metode bagging (atau pasting), dengan `max_samples` = ukuran training set.\n",
        "\n",
        "**Keunggulan:**\n",
        "* Sangat cepat dan scalable.\n",
        "* Mengukur **Feature Importance**: Menghitung seberapa banyak error meningkat ketika data untuk fitur tertentu diacak (permutation importance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≤ Extra-Trees\n",
        "\n",
        "Extra-Trees (Extremely Randomized Trees) adalah varian dari Random Forest yang lebih acak.\n",
        "\n",
        "* **Perbedaan Utama**: Saat membelah node, Extra-Trees menggunakan **threshold acak** (alih-alih mencari threshold optimal seperti Random Forest).\n",
        "* **Keuntungan**: Lebih cepat dilatih (mencari threshold optimal memakan waktu).\n",
        "* **Trade-off**: Menukar bias lebih tinggi dengan variance lebih rendah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "et_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Feature Importance\n",
        "\n",
        "Random Forests bisa mengukur **pentingnya relatif** setiap fitur.\n",
        "\n",
        "* **Cara Kerja**: Mengukur seberapa banyak error meningkat ketika data untuk fitur itu diacak.\n",
        "* **Keuntungan**: Memberi wawasan cepat tentang fitur paling penting di dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Contoh dengan dataset Iris\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf_iris = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf_iris.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf_iris.feature_importances_):\n",
        "    print(f\"{name}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Boosting\n",
        "\n",
        "Boosting adalah metode ensemble sekuensial yang mencoba memperbaiki kesalahan dari prediktor sebelumnya.\n",
        "\n",
        "### AdaBoost (Adaptive Boosting)\n",
        "\n",
        "* **Cara Kerja**: Melatih prediktor pertama, lalu meningkatkan bobot instance yang *underfitted* (kesalahan prediksi). Prediktor kedua fokus lebih pada instance sulit itu, dan seterusnya.\n",
        "* **Hyperparameters**: `n_estimators`, `learning_rate` (shrinkage).\n",
        "* **Prediksi**: Rata-rata berbobot dari prediksi prediktor (prediktor lebih akurat punya bobot lebih tinggi).\n",
        "* **SAMME & SAMME.R**: SAMME.R menggunakan probabilitas kelas (lebih baik)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "* **Cara Kerja**: Mirip AdaBoost, tapi fokus pada **residual errors** (selisih prediksi vs. target) dari prediktor sebelumnya.\n",
        "* **Early Stopping**: Gunakan untuk mencari jumlah optimal pohon (`staged_predict()`).\n",
        "* **Shrinkage**: Mengurangi kontribusi setiap pohon (`learning_rate` rendah = regularisasi).\n",
        "* **Stochastic GB**: Subsampling instance (`subsample=0.25`) untuk kecepatan dan regularisasi.\n",
        "* **XGBoost**: Implementasi GB yang dioptimasi, sangat cepat dan populer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# GBRT dengan early stopping\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü•û Stacking\n",
        "\n",
        "Stacking menggunakan **blender** (atau meta-learner) untuk mempelajari cara terbaik menggabungkan prediksi dari prediktor base.\n",
        "\n",
        "* **Cara Kerja**: Bagi training set menjadi subset. Latih base prediktor pada subset 1, lalu gunakan prediksi mereka di subset 2 sebagai input untuk blender.\n",
        "* **Multi-Layer Stacking**: Anda bisa punya beberapa layer blender.\n",
        "* **Scikit-Learn**: Gunakan `StackingClassifier` atau implementasikan manual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('lr', LogisticRegression()), ('rf', RandomForestClassifier()), ('svc', SVC())],\n",
        "    final_estimator=RandomForestClassifier()\n",
        ")\n",
        "stacking_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Kesimpulan Chapter 7\n",
        "\n",
        "### üìä Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CHAPTER 7 SUMMARY - ENSEMBLE METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_table = \"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Method             ‚îÇ Key Idea             ‚îÇ When to Use                ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Voting Classifier  ‚îÇ Majority vote        ‚îÇ Diverse classifiers        ‚îÇ\n",
        "‚îÇ                    ‚îÇ (hard/soft)          ‚îÇ Soft voting if possible    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Bagging            ‚îÇ Subset random        ‚îÇ Reduce variance            ‚îÇ\n",
        "‚îÇ                    ‚îÇ (w/ replacement)     ‚îÇ Parallel training          ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Pasting            ‚îÇ Subset random        ‚îÇ Similar to bagging         ‚îÇ\n",
        "‚îÇ                    ‚îÇ (w/o replacement)    ‚îÇ Smaller datasets           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Random Forest      ‚îÇ Bagging + Trees      ‚îÇ General purpose            ‚îÇ\n",
        "‚îÇ                    ‚îÇ Random features      ‚îÇ Feature importance         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Extra-Trees        ‚îÇ RF + Random splits   ‚îÇ Faster training            ‚îÇ\n",
        "‚îÇ                    ‚îÇ                      ‚îÇ Slightly higher bias       ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ AdaBoost           ‚îÇ Sequential           ‚îÇ Weak learners              ‚îÇ\n",
        "‚îÇ                    ‚îÇ Focus on errors      ‚îÇ SAMME.R for probabilities  ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Gradient Boosting  ‚îÇ Sequential           ‚îÇ Trees as base              ‚îÇ\n",
        "‚îÇ                    ‚îÇ Fit residuals        ‚îÇ Early stopping             ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Stacking           ‚îÇ Meta-learner         ‚îÇ Complex ensembles          ‚îÇ\n",
        "‚îÇ                    ‚îÇ (blender)            ‚îÇ Multi-layer possible       ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\"\n",
        "\n",
        "print(summary_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Key Takeaways\n",
        "\n",
        "**1. Ensemble > Single:**\n",
        "- Gabungkan model lemah ‚Üí model kuat.\n",
        "- Keragaman (diversity) adalah kunci sukses.\n",
        "\n",
        "**2. Bagging/ Random Forest:**\n",
        "- Reduce variance dengan sampling acak.\n",
        "- OOB untuk validasi gratis.\n",
        "- Feature importance untuk insights.\n",
        "\n",
        "**3. Boosting:**\n",
        "- Sequential: Fokus pada kesalahan.\n",
        "- AdaBoost: Adjust bobot instance.\n",
        "- Gradient Boosting: Fit residuals.\n",
        "- Gunakan shrinkage & early stopping.\n",
        "\n",
        "**4. Stacking:**\n",
        "- Gunakan blender untuk gabungkan prediksi.\n",
        "- Lebih fleksibel untuk ensemble kompleks.\n",
        "\n",
        "**5. Parallel vs Sequential:**\n",
        "- Bagging/RF/Stacking: Parallel (cepat).\n",
        "- Boosting: Sequential (lambat tapi kuat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Exercises (from the book)\n",
        "\n",
        "### Exercise 1\n",
        "**Q:** If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results?\n",
        "**A:** Ya, ada peluang. Jika kelima model **membuat tipe kesalahan yang berbeda**, menggabungkan mereka dengan *voting classifier* (terutama soft voting) dapat meningkatkan akurasi/presisi. Jika mereka semua membuat kesalahan yang *sama persis*, menggabungkannya tidak akan membantu.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2\n",
        "**Q:** What is the difference between hard and soft voting classifiers?\n",
        "**A:**\n",
        "* **Hard voting**: Menghitung suara dari setiap classifier dan memilih kelas yang paling banyak dipilih (mode statistik).\n",
        "* **Soft voting**: Menghitung rata-rata probabilitas kelas yang diprediksi dari semua classifier, lalu memilih kelas dengan probabilitas rata-rata tertinggi. Soft voting seringkali lebih baik karena memberi bobot lebih pada suara yang sangat \"percaya diri\".\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3\n",
        "**Q:** Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting, boosting, Random Forests, or stacking?\n",
        "**A:**\n",
        "* **Bagging:** **Ya**. Setiap prediktor dilatih secara independen pada subset data yang berbeda, sehingga dapat diparalelkan dengan sempurna.\n",
        "* **Pasting:** **Ya**. Sama seperti bagging, pelatihan bersifat independen dan paralel.\n",
        "* **Boosting:** **Tidak**. Boosting bersifat *sekuensial*; setiap prediktor baru bergantung pada hasil prediktor sebelumnya, sehingga tidak dapat diparalelkan.\n",
        "* **Random Forests:** **Ya**. Sama seperti bagging, setiap pohon dilatih secara independen.\n",
        "* **Stacking:** **Sebagian**. Prediktor-prediktor dalam *satu layer* dapat dilatih secara paralel. Namun, *antar layer* bersifat sekuensial (Layer 2 harus menunggu Layer 1 selesai, Blender harus menunggu Layer 1 selesai membuat prediksi).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4\n",
        "**Q:** What is the benefit of out-of-bag evaluation?\n",
        "**A:** Manfaat utamanya adalah Anda mendapatkan evaluasi model (mirip dengan cross-validation) secara **gratis** tanpa perlu membuat *validation set* terpisah. Ini karena setiap prediktor dievaluasi pada instance (oob) yang tidak pernah dilihatnya selama pelatihan.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5\n",
        "**Q:** What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster?\n",
        "**A:**\n",
        "* **Lebih Acak**: Extra-Trees menggunakan *threshold* (ambang batas) acak untuk membelah fitur, sedangkan Random Forest mencari *threshold* optimal.\n",
        "* **Manfaat**: Keacakan ekstra ini adalah bentuk *regularisasi* lain. Ini menukar bias yang sedikit lebih tinggi dengan variance yang lebih rendah.\n",
        "* **Kecepatan**: Extra-Trees **lebih cepat** dilatih, karena mencari threshold optimal (yang dilakukan Random Forest) adalah salah satu tugas yang paling memakan waktu dalam menumbuhkan pohon.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6\n",
        "**Q:** If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?\n",
        "**A:** Untuk mengurangi underfitting (meningkatkan kompleksitas):\n",
        "1.  **`n_estimators`**: **Naikkan** jumlah estimator (pohon).\n",
        "2.  **`learning_rate`**: **Naikkan** learning rate (default 1.0).\n",
        "3.  **Regularisasi Base Estimator**: **Kurangi** regularisasi pada base estimator (misalnya, jika menggunakan `DecisionTreeClassifier`, *naikkan* `max_depth` dari 1).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 7\n",
        "**Q:** If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
        "**A:** Anda harus **mengurangi** `learning_rate`. `learning_rate` yang lebih rendah (shrinkage) berarti setiap pohon memiliki kontribusi yang lebih kecil, yang meregularisasi model. Namun, Anda mungkin perlu *menaikkan* `n_estimators` (menggunakan early stopping) untuk mengimbanginya.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8 & 9\n",
        "**Q:** (MNIST Stacking)\n",
        "**A:** (Requires implementation)\n",
        "* **Ex. 8:** Latih beberapa classifier (Random Forest, Extra-Trees, SVM) di MNIST. Gabungkan mereka menggunakan `VotingClassifier` (coba 'hard' dan 'soft'). Evaluasi di validation set, lalu test set, dan bandingkan hasilnya.\n",
        "* **Ex. 9:** Gunakan classifier dari Ex. 8. Buat prediksi dari *validation set* untuk membuat *blending set* baru. Latih \"blender\" (misalnya, `RandomForestClassifier` lain) pada blending set ini. Evaluasi ensemble stacking ini di test set. Bandingkan hasilnya dengan Voting Classifier dari Ex. 8.\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Ensembling! üå≤üå≤üå≤**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}