{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Classification\n",
        "\n",
        "---\n",
        "\n",
        "## Rangkuman Chapter 3\n",
        "\n",
        "Chapter ini membahas **Classification Tasks** menggunakan **MNIST Dataset** - dataset yang berisi 70,000 gambar tulisan tangan angka (0-9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Jenis-Jenis Classification\n",
        "\n",
        "| Tipe | Deskripsi | Contoh |\n",
        "|------|-----------|--------|\n",
        "| **Binary Classification** | Membedakan 2 kelas | 5 vs bukan-5 |\n",
        "| **Multiclass Classification** | Membedakan >2 kelas | Angka 0-9 |\n",
        "| **Multilabel Classification** | Multiple binary tags per instance | Gambar dengan banyak objek |\n",
        "| **Multioutput Classification** | Multiple multiclass labels | Image denoising |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Metrics\n",
        "\n",
        "### Confusion Matrix\n",
        "\n",
        "```\n",
        "                Predicted\n",
        "                Negative  Positive\n",
        "Actual Negative    TN        FP\n",
        "       Positive    FN        TP\n",
        "```\n",
        "\n",
        "**Komponen:**\n",
        "- **TN (True Negative)**: Correctly predicted negative\n",
        "- **FP (False Positive)**: Incorrectly predicted positive (Type I Error)\n",
        "- **FN (False Negative)**: Incorrectly predicted negative (Type II Error)\n",
        "- **TP (True Positive)**: Correctly predicted positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision\n",
        "\n",
        "Akurasi dari positive predictions:\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "**Kapan penting?**\n",
        "- Ketika False Positive mahal\n",
        "- Contoh: Spam filter, video safety classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recall (Sensitivity / TPR)\n",
        "\n",
        "Ratio positive instances yang terdeteksi:\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "**Kapan penting?**\n",
        "- Ketika False Negative mahal\n",
        "- Contoh: Cancer detection, shoplifter detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F₁ Score\n",
        "\n",
        "Harmonic mean dari Precision dan Recall:\n",
        "\n",
        "$$F_1 = \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$\n",
        "\n",
        "**Karakteristik:**\n",
        "- F₁ tinggi hanya jika Precision DAN Recall keduanya tinggi\n",
        "- Harmonic mean memberi bobot lebih pada nilai rendah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC Curve & AUC\n",
        "\n",
        "**ROC (Receiver Operating Characteristic):**\n",
        "- Plot TPR (Recall) vs FPR (False Positive Rate)\n",
        "- FPR = 1 - Specificity = FP / (FP + TN)\n",
        "\n",
        "**AUC (Area Under the Curve):**\n",
        "- Perfect classifier: AUC = 1\n",
        "- Random classifier: AUC = 0.5\n",
        "\n",
        "**Kapan menggunakan ROC vs PR Curve?**\n",
        "\n",
        "| Kondisi | Gunakan |\n",
        "|---------|---------|\n",
        "| Positive class jarang | **PR Curve** |\n",
        "| Peduli pada False Positives | **PR Curve** |\n",
        "| Balanced classes | **ROC Curve** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision/Recall Trade-off\n",
        "\n",
        "```\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│  Threshold ↑  →  Precision ↑  &  Recall ↓              │\n",
        "│  Threshold ↓  →  Precision ↓  &  Recall ↑              │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Strategi pemilihan threshold:**\n",
        "1. Plot Precision vs Recall curve\n",
        "2. Tentukan requirement bisnis\n",
        "3. Pilih threshold yang sesuai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiclass Classification Strategies\n",
        "\n",
        "### One-vs-Rest (OvR / One-vs-All)\n",
        "\n",
        "```\n",
        "Train N binary classifiers (N = jumlah kelas)\n",
        "- Classifier 0: \"0\" vs \"not 0\"\n",
        "- Classifier 1: \"1\" vs \"not 1\"\n",
        "- ...\n",
        "- Classifier 9: \"9\" vs \"not 9\"\n",
        "\n",
        "Prediction: Pilih kelas dengan score tertinggi\n",
        "```\n",
        "\n",
        "### One-vs-One (OvO)\n",
        "\n",
        "```\n",
        "Train N×(N-1)/2 binary classifiers\n",
        "- Classifier 0-1: \"0\" vs \"1\"\n",
        "- Classifier 0-2: \"0\" vs \"2\"\n",
        "- ...\n",
        "- Classifier 8-9: \"8\" vs \"9\"\n",
        "\n",
        "Prediction: Pilih kelas yang menang paling banyak\n",
        "```\n",
        "\n",
        "**Kapan menggunakan?**\n",
        "- **OvO**: Untuk algoritma yang scale poorly (e.g., SVM)\n",
        "- **OvR**: Untuk algoritma lainnya (lebih umum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementasi Kode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 1: Setup dan Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8-darkgrid')\n\nprint(\"Libraries imported!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fetch MNIST dataset\nprint(\"Downloading MNIST dataset...\")\nmnist = fetch_openml('mnist_784', version=1, parser='auto')\nprint(\"Dataset loaded!\")\n\n# Explore dataset structure\nprint(\"\\n\" + \"=\"*80)\nprint(\"MNIST DATASET STRUCTURE\")\nprint(\"=\"*80)\nprint(f\"Keys: {mnist.keys()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 2: Explore MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get features and labels\nX, y = mnist[\"data\"], mnist[\"target\"]\n\nprint(\"=\"*80)\nprint(\"DATASET SHAPE\")\nprint(\"=\"*80)\nprint(f\"Features (X): {X.shape}\")\nprint(f\"Labels (y):   {y.shape}\")\nprint(f\"\\nTotal images: {X.shape[0]:,}\")\nprint(f\"Pixels per image: {X.shape[1]} (28×28)\")\nprint(f\"Pixel intensity: 0 (white) to 255 (black)\")\n\n# Convert labels to integers\ny = y.astype(np.uint8)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"LABEL DISTRIBUTION\")\nprint(\"=\"*80)\nunique, counts = np.unique(y, return_counts=True)\nfor digit, count in zip(unique, counts):\n    print(f\"Digit {digit}: {count:>5,} images ({count/len(y)*100:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 3: Visualize MNIST Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_digit(data, label=None):\n    \"\"\"Plot a single digit\"\"\"\n    image = data.reshape(28, 28)\n    plt.imshow(image, cmap='binary')\n    plt.axis('off')\n    if label is not None:\n        plt.title(f'Label: {label}', fontsize=14, fontweight='bold')\n\ndef plot_digits(instances, images_per_row=10, **options):\n    \"\"\"Plot multiple digits in a grid\"\"\"\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    n_rows = (len(instances) - 1) // images_per_row + 1\n    \n    n_empty = n_rows * images_per_row - len(instances)\n    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n    \n    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size, images_per_row * size)\n    \n    plt.imshow(big_image, cmap='binary', **options)\n    plt.axis('off')\n\n# Visualize first image\nsome_digit = X.iloc[0] if hasattr(X, 'iloc') else X[0]\nsome_label = y[0]\n\nplt.figure(figsize=(6, 6))\nplot_digit(some_digit, some_label)\nplt.show()\n\nprint(f\"First image label: {some_label}\")\n\n# Visualize multiple images\nplt.figure(figsize=(12, 12))\nexample_images = X[:100] if hasattr(X, 'iloc') else X[:100]\nplot_digits(np.array(example_images), images_per_row=10)\nplt.suptitle('100 MNIST Images', fontsize=16, fontweight='bold', y=0.98)\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 4: Split Train/Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MNIST is already split: first 60k = train, last 10k = test\nX_train, X_test = X[:60000], X[60000:]\ny_train, y_test = y[:60000], y[60000:]\n\nprint(\"=\"*80)\nprint(\"TRAIN/TEST SPLIT\")\nprint(\"=\"*80)\nprint(f\"Training set:   {len(X_train):>6,} images\")\nprint(f\"Test set:       {len(X_test):>6,} images\")\nprint(f\"Test ratio:     {len(X_test)/len(X)*100:>6.1f}%\")\n\nprint(\"\\nTraining set already shuffled (good for cross-validation)\")\nprint(\"Never look at test set until final evaluation!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 5: Binary Classification - Detect 5s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import SGDClassifier\n\n# Create binary target: True for 5s, False for others\ny_train_5 = (y_train == 5)\ny_test_5 = (y_test == 5)\n\nprint(\"=\"*80)\nprint(\"BINARY CLASSIFICATION: 5-DETECTOR\")\nprint(\"=\"*80)\nprint(f\"Training samples:\")\nprint(f\"  Class '5':     {y_train_5.sum():>6,} ({y_train_5.sum()/len(y_train_5)*100:.1f}%)\")\nprint(f\"  Class 'not-5': {(~y_train_5).sum():>6,} ({(~y_train_5).sum()/len(y_train_5)*100:.1f}%)\")\n\n# Train SGD Classifier\nsgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)\nsgd_clf.fit(X_train, y_train_5)\n\nprint(\"\\nSGDClassifier trained!\")\n\n# Test prediction\nprediction = sgd_clf.predict([some_digit])\nprint(f\"\\nPrediction for first image (label={some_label}):\")\nprint(f\"  Is it a 5? {prediction[0]}\")\nprint(f\"  {'Correct!' if (prediction[0] and some_label == 5) or (not prediction[0] and some_label != 5) else 'Wrong!'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 6: Cross-Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate using cross-validation\nprint(\"=\"*80)\nprint(\"CROSS-VALIDATION EVALUATION\")\nprint(\"=\"*80)\nprint(\"Performing 3-fold cross-validation...\")\n\nscores = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\nprint(f\"\\nAccuracy scores: {scores}\")\nprint(f\"Mean accuracy:   {scores.mean():.4f}\")\nprint(f\"Std deviation:   {scores.std():.4f}\")\n\n# Compare with dummy classifier\nfrom sklearn.base import BaseEstimator\n\nclass Never5Classifier(BaseEstimator):\n    \"\"\"Dummy classifier that always predicts 'not-5'\"\"\"\n    def fit(self, X, y=None):\n        return self\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n\nnever_5_clf = Never5Classifier()\ndummy_scores = cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPARISON WITH DUMMY CLASSIFIER\")\nprint(\"=\"*80)\nprint(f\"Dummy classifier (always predicts 'not-5'):\")\nprint(f\"  Accuracy: {dummy_scores.mean():.4f}\")\nprint(f\"\\nHigh accuracy doesn't mean good classifier!\")\nprint(f\"    Only ~{y_train_5.sum()/len(y_train_5)*100:.0f}% are 5s, so guessing 'not-5' gives ~{dummy_scores.mean()*100:.0f}% accuracy\")\nprint(f\"\\nNeed better metrics: Precision, Recall, F₁ Score\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 7: Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get predictions using cross-validation\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n\n# Compute confusion matrix\nconf_mx = confusion_matrix(y_train_5, y_train_pred)\n\nprint(\"=\"*80)\nprint(\"CONFUSION MATRIX\")\nprint(\"=\"*80)\nprint(conf_mx)\n\n# Extract values\ntn, fp, fn, tp = conf_mx.ravel()\n\nprint(f\"\\n{'Component':<20} {'Count':>10} {'Description'}\")\nprint(\"-\"*80)\nprint(f\"{'True Negatives (TN)':<20} {tn:>10,}   Correctly predicted as 'not-5'\")\nprint(f\"{'False Positives (FP)':<20} {fp:>10,}   Wrongly predicted as '5'\")\nprint(f\"{'False Negatives (FN)':<20} {fn:>10,}   Wrongly predicted as 'not-5'\")\nprint(f\"{'True Positives (TP)':<20} {tp:>10,}   Correctly predicted as '5'\")\n\n# Visualize confusion matrix\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Raw confusion matrix\naxes[0].matshow(conf_mx, cmap='Blues', alpha=0.7)\nfor i in range(2):\n    for j in range(2):\n        axes[0].text(j, i, conf_mx[i, j], ha='center', va='center', \n                    fontsize=20, fontweight='bold')\naxes[0].set_xlabel('Predicted', fontsize=12)\naxes[0].set_ylabel('Actual', fontsize=12)\naxes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\naxes[0].set_xticks([0, 1])\naxes[0].set_yticks([0, 1])\naxes[0].set_xticklabels(['not-5', '5'])\naxes[0].set_yticklabels(['not-5', '5'])\n\n# Normalized confusion matrix\nconf_mx_norm = conf_mx / conf_mx.sum(axis=1, keepdims=True)\naxes[1].matshow(conf_mx_norm, cmap='Blues', alpha=0.7)\nfor i in range(2):\n    for j in range(2):\n        axes[1].text(j, i, f'{conf_mx_norm[i, j]:.2%}', ha='center', va='center',\n                    fontsize=16, fontweight='bold')\naxes[1].set_xlabel('Predicted', fontsize=12)\naxes[1].set_ylabel('Actual', fontsize=12)\naxes[1].set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')\naxes[1].set_xticks([0, 1])\naxes[1].set_yticks([0, 1])\naxes[1].set_xticklabels(['not-5', '5'])\naxes[1].set_yticklabels(['not-5', '5'])\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 8: Precision, Recall, and F₁ Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate metrics\nprecision = precision_score(y_train_5, y_train_pred)\nrecall = recall_score(y_train_5, y_train_pred)\nf1 = f1_score(y_train_5, y_train_pred)\n\nprint(\"=\"*80)\nprint(\"CLASSIFICATION METRICS\")\nprint(\"=\"*80)\nprint(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\nprint(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\nprint(f\"F₁ Score:  {f1:.4f}\")\n\n# Manual calculation for verification\nprint(\"\\n\" + \"=\"*80)\nprint(\"MANUAL CALCULATION (Verification)\")\nprint(\"=\"*80)\nprint(f\"Precision = TP / (TP + FP) = {tp} / ({tp} + {fp}) = {tp/(tp+fp):.4f}\")\nprint(f\"Recall    = TP / (TP + FN) = {tp} / ({tp} + {fn}) = {tp/(tp+fn):.4f}\")\nprint(f\"F₁        = 2 × (P × R) / (P + R) = {2*precision*recall/(precision+recall):.4f}\")\n\n# Interpretation\nprint(\"\\n\" + \"=\"*80)\nprint(\"INTERPRETATION\")\nprint(\"=\"*80)\nprint(f\"• When classifier says '5': correct {precision*100:.1f}% of time\")\nprint(f\"• Detects {recall*100:.1f}% of all actual 5s\")\nprint(f\"• F₁ score balances both metrics\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 9: Precision/Recall Trade-off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import precision_recall_curve\n\n# Get decision scores\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, \n                             method=\"decision_function\")\n\n# Compute precision and recall for all thresholds\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\n# Plot Precision and Recall vs Threshold\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Precision & Recall vs Threshold\naxes[0].plot(thresholds, precisions[:-1], 'b--', label='Precision', linewidth=2)\naxes[0].plot(thresholds, recalls[:-1], 'g-', label='Recall', linewidth=2)\naxes[0].axvline(x=0, color='red', linestyle=':', alpha=0.7, label='Default threshold')\naxes[0].set_xlabel('Threshold', fontsize=12)\naxes[0].set_ylabel('Score', fontsize=12)\naxes[0].set_title('Precision & Recall vs Threshold', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_xlim([-50000, 50000])\n\n# Plot 2: Precision vs Recall\naxes[1].plot(recalls, precisions, 'b-', linewidth=2)\naxes[1].set_xlabel('Recall', fontsize=12)\naxes[1].set_ylabel('Precision', fontsize=12)\naxes[1].set_title('Precision vs Recall', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Observations:\")\nprint(\"• Increasing threshold → Precision ↑, Recall ↓\")\nprint(\"• Decreasing threshold → Precision ↓, Recall ↑\")\nprint(\"• Choose threshold based on business requirements\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 10: Select Threshold for Target Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Target: 90% precision\ntarget_precision = 0.90\n\n# Find threshold\nidx = np.argmax(precisions >= target_precision)\nthreshold_90 = thresholds[idx]\nprecision_90 = precisions[idx]\nrecall_90 = recalls[idx]\n\nprint(\"=\"*80)\nprint(\"THRESHOLD SELECTION FOR 90% PRECISION\")\nprint(\"=\"*80)\nprint(f\"Target precision:  {target_precision:.0%}\")\nprint(f\"Selected threshold: {threshold_90:,.2f}\")\nprint(f\"Actual precision:  {precision_90:.4f} ({precision_90*100:.2f}%)\")\nprint(f\"Recall at this threshold: {recall_90:.4f} ({recall_90*100:.2f}%)\")\n\n# Make predictions with new threshold\ny_train_pred_90 = (y_scores >= threshold_90)\n\n# Verify\nprecision_check = precision_score(y_train_5, y_train_pred_90)\nrecall_check = recall_score(y_train_5, y_train_pred_90)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"VERIFICATION\")\nprint(\"=\"*80)\nprint(f\"Precision: {precision_check:.4f}\")\nprint(f\"Recall:    {recall_check:.4f}\")\n\nprint(\"\\nTrade-off:\")\nprint(f\"  Precision ↑ from {precision*100:.1f}% to {precision_check*100:.1f}%\")\nprint(f\"  Recall ↓ from {recall*100:.1f}% to {recall_check*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 11: ROC Curve and AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n\n# Compute ROC curve\nfpr, tpr, roc_thresholds = roc_curve(y_train_5, y_scores)\n\n# Compute AUC\nroc_auc = roc_auc_score(y_train_5, y_scores)\n\n# Plot ROC curve\nplt.figure(figsize=(10, 7))\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'SGD (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n\n# Highlight point at recall ~43.7%\nidx_recall = np.argmin(np.abs(tpr - recall_90))\nplt.plot(fpr[idx_recall], tpr[idx_recall], 'ro', markersize=10, \n         label=f'90% Precision point\\n(Recall={tpr[idx_recall]:.2%})')\n\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.title('ROC Curve', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"=\"*80)\nprint(\"ROC AUC SCORE\")\nprint(\"=\"*80)\nprint(f\"AUC: {roc_auc:.4f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"• Perfect classifier: AUC = 1.0\")\nprint(f\"• Random classifier:  AUC = 0.5\")\nprint(f\"• Our classifier:     AUC = {roc_auc:.4f} {'Good!' if roc_auc > 0.9 else 'Could be better'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 12: Compare with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n\n# Train Random Forest\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, \n                                    method=\"predict_proba\")\n\n# Get probabilities for positive class\ny_scores_forest = y_probas_forest[:, 1]\n\n# Compute ROC curve\nfpr_forest, tpr_forest, _ = roc_curve(y_train_5, y_scores_forest)\nroc_auc_forest = roc_auc_score(y_train_5, y_scores_forest)\n\n# Plot comparison\nplt.figure(figsize=(10, 7))\nplt.plot(fpr, tpr, 'b:', linewidth=2, label=f'SGD (AUC = {roc_auc:.4f})')\nplt.plot(fpr_forest, tpr_forest, 'g-', linewidth=2, \n         label=f'Random Forest (AUC = {roc_auc_forest:.4f})')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random Classifier')\n\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11, loc='lower right')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"=\"*80)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*80)\nprint(f\"{'Model':<20} {'AUC':>10}\")\nprint(\"-\"*35)\nprint(f\"{'SGD Classifier':<20} {roc_auc:>10.4f}\")\nprint(f\"{'Random Forest':<20} {roc_auc_forest:>10.4f}\")\n\nprint(f\"\\nRandom Forest is better (closer to top-left corner)\")\n\n# Compute precision and recall for Random Forest\ny_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\nprecision_forest = precision_score(y_train_5, y_train_pred_forest)\nrecall_forest = recall_score(y_train_5, y_train_pred_forest)\n\nprint(f\"\\nRandom Forest Performance:\")\nprint(f\"  Precision: {precision_forest:.4f} ({precision_forest*100:.1f}%)\")\nprint(f\"  Recall:    {recall_forest:.4f} ({recall_forest*100:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 13: Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n\n# Train SVM on full dataset (0-9)\nprint(\"=\"*80)\nprint(\"MULTICLASS CLASSIFICATION (Digits 0-9)\")\nprint(\"=\"*80)\nprint(\"Training SVM classifier...\")\n\nsvm_clf = SVC(gamma='auto', random_state=42)\nsvm_clf.fit(X_train[:5000], y_train[:5000])  # Use subset for speed\n\nprint(\"SVM trained!\")\n\n# Make prediction\nprediction = svm_clf.predict([some_digit])\nprint(f\"\\nPrediction for first image: {prediction[0]}\")\nprint(f\"Actual label: {some_label}\")\nprint(f\"{'Correct!' if prediction[0] == some_label else 'Wrong!'}\")\n\n# Get decision scores\nsome_digit_scores = svm_clf.decision_function([some_digit])\nprint(f\"\\n{'='*80}\")\nprint(\"DECISION SCORES FOR EACH CLASS\")\nprint(\"=\"*80)\nfor i, score in enumerate(some_digit_scores[0]):\n    marker = \" ← HIGHEST\" if i == np.argmax(some_digit_scores) else \"\"\n    print(f\"Class {i}: {score:>10.4f}{marker}\")\n\nprint(f\"\\nSVM uses One-vs-One (OvO) strategy:\")\nprint(f\"   Trained {len(svm_clf.classes_)*(len(svm_clf.classes_)-1)//2} binary classifiers\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 14: SGD Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\n# Scale features\nprint(\"=\"*80)\nprint(\"SGD MULTICLASS CLASSIFICATION WITH SCALING\")\nprint(\"=\"*80)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n\n# Train SGD on scaled data\nsgd_clf_multi = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)\nsgd_clf_multi.fit(X_train_scaled, y_train)\n\nprint(\"SGD classifier trained!\")\n\n# Evaluate with cross-validation\nscores = cross_val_score(sgd_clf_multi, X_train_scaled, y_train, \n                        cv=3, scoring=\"accuracy\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"CROSS-VALIDATION RESULTS\")\nprint(\"=\"*80)\nprint(f\"Accuracy scores: {scores}\")\nprint(f\"Mean accuracy:   {scores.mean():.4f} ({scores.mean()*100:.2f}%)\")\nprint(f\"Std deviation:   {scores.std():.4f}\")\n\nprint(f\"\\nMuch better than random guessing (10%)!\")\nprint(f\"Feature scaling improved performance significantly\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 15: Multiclass Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get predictions\ny_train_pred_multi = cross_val_predict(sgd_clf_multi, X_train_scaled, y_train, cv=3)\n\n# Compute confusion matrix\nconf_mx_multi = confusion_matrix(y_train, y_train_pred_multi)\n\nprint(\"=\"*80)\nprint(\"MULTICLASS CONFUSION MATRIX\")\nprint(\"=\"*80)\nprint(conf_mx_multi)\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Raw confusion matrix\nim1 = axes[0].matshow(conf_mx_multi, cmap='Blues')\naxes[0].set_xlabel('Predicted', fontsize=12)\naxes[0].set_ylabel('Actual', fontsize=12)\naxes[0].set_title('Confusion Matrix (Raw Counts)', fontsize=14, fontweight='bold')\nplt.colorbar(im1, ax=axes[0])\n\n# Normalized confusion matrix (focus on errors)\nrow_sums = conf_mx_multi.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx_multi / row_sums\nnp.fill_diagonal(norm_conf_mx, 0)  # Remove diagonal to focus on errors\n\nim2 = axes[1].matshow(norm_conf_mx, cmap='Blues')\naxes[1].set_xlabel('Predicted', fontsize=12)\naxes[1].set_ylabel('Actual', fontsize=12)\naxes[1].set_title('Error Rates (Diagonal Removed)', fontsize=14, fontweight='bold')\nplt.colorbar(im2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nAnalysis:\")\nprint(\"• Bright columns → Many false predictions to that class\")\nprint(\"• Bright rows → That class is often misclassified\")\nprint(\"• Example: 3s and 5s often confused with each other\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 16: Error Analysis - 3s vs 5s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze confusion between 3s and 5s\ncl_a, cl_b = 3, 5\n\nX_aa = X_train[(y_train == cl_a) & (y_train_pred_multi == cl_a)]\nX_ab = X_train[(y_train == cl_a) & (y_train_pred_multi == cl_b)]\nX_ba = X_train[(y_train == cl_b) & (y_train_pred_multi == cl_a)]\nX_bb = X_train[(y_train == cl_b) & (y_train_pred_multi == cl_b)]\n\nprint(\"=\"*80)\nprint(f\"ERROR ANALYSIS: {cl_a}s vs {cl_b}s\")\nprint(\"=\"*80)\nprint(f\"Correctly classified as {cl_a}: {len(X_aa):>5,}\")\nprint(f\"{cl_a}s misclassified as {cl_b}:  {len(X_ab):>5,}\")\nprint(f\"{cl_b}s misclassified as {cl_a}:  {len(X_ba):>5,}\")\nprint(f\"Correctly classified as {cl_b}: {len(X_bb):>5,}\")\n\n# Visualize confusion cases\nfig = plt.figure(figsize=(10, 10))\n\nplt.subplot(221)\nplot_digits(np.array(X_aa[:25]), images_per_row=5)\nplt.title(f'Classified as {cl_a} (Correct)', fontsize=12, fontweight='bold')\n\nplt.subplot(222)\nplot_digits(np.array(X_ab[:25]), images_per_row=5)\nplt.title(f'{cl_a}s classified as {cl_b}s (Error)', fontsize=12, fontweight='bold', color='red')\n\nplt.subplot(223)\nplot_digits(np.array(X_ba[:25]), images_per_row=5)\nplt.title(f'{cl_b}s classified as {cl_a}s (Error)', fontsize=12, fontweight='bold', color='red')\n\nplt.subplot(224)\nplot_digits(np.array(X_bb[:25]), images_per_row=5)\nplt.title(f'Classified as {cl_b} (Correct)', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInsights:\")\nprint(f\"• Many {cl_a}s and {cl_b}s are similar in appearance\")\nprint(\"• Linear models sensitive to small differences\")\nprint(\"• Solution: Better preprocessing, feature engineering, or CNN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 17: Multilabel Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n\n# Create multilabel targets\ny_train_large = (y_train >= 7)  # True if digit is 7, 8, or 9\ny_train_odd = (y_train % 2 == 1)  # True if digit is odd\ny_multilabel = np.c_[y_train_large, y_train_odd]\n\nprint(\"=\"*80)\nprint(\"MULTILABEL CLASSIFICATION\")\nprint(\"=\"*80)\nprint(\"Two labels per instance:\")\nprint(\"  1. Is digit large? (≥7)\")\nprint(\"  2. Is digit odd?\")\n\n# Train KNN classifier\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_multilabel)\n\nprint(\"\\nKNN classifier trained!\")\n\n# Make prediction\nprediction = knn_clf.predict([some_digit])\nprint(f\"\\nPrediction for digit {some_label}:\")\nprint(f\"  Is large (≥7)? {prediction[0][0]}\")\nprint(f\"  Is odd?        {prediction[0][1]}\")\n\n# Verify\nactual_large = some_label >= 7\nactual_odd = some_label % 2 == 1\nprint(f\"\\nActual:\")\nprint(f\"  Is large (≥7)? {actual_large}\")\nprint(f\"  Is odd?        {actual_odd}\")\n\nprint(f\"\\n{'Correct!' if np.array_equal(prediction[0], [actual_large, actual_odd]) else 'Wrong!'}\")\n\n# Evaluate with F1 score\ny_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\nf1_multilabel = f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"MULTILABEL EVALUATION\")\nprint(\"=\"*80)\nprint(f\"F₁ Score (macro average): {f1_multilabel:.4f}\")\nprint(f\"\\n'macro' average: Equal weight to all labels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cell 18: Multioutput Classification - Image Denoising"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add noise to images\nnp.random.seed(42)\nnoise = np.random.randint(0, 100, (len(X_train), 784))\nX_train_mod = X_train + noise\nnoise_test = np.random.randint(0, 100, (len(X_test), 784))\nX_test_mod = X_test + noise_test\n\n# Target is the original clean image\ny_train_mod = X_train\ny_test_mod = X_test\n\nprint(\"=\"*80)\nprint(\"MULTIOUTPUT CLASSIFICATION: IMAGE DENOISING\")\nprint(\"=\"*80)\nprint(\"Task: Remove noise from images\")\nprint(f\"  Input:  Noisy image (28×28 = 784 pixel intensities)\")\nprint(f\"  Output: Clean image (784 pixel intensities)\")\nprint(f\"  Type:   Multioutput (784 outputs, each 0-255)\")\n\n# Visualize noisy vs clean\nsome_index = 0\nnoisy_digit = X_test_mod.iloc[some_index] if hasattr(X_test_mod, 'iloc') else X_test_mod[some_index]\nclean_digit = y_test_mod.iloc[some_index] if hasattr(y_test_mod, 'iloc') else y_test_mod[some_index]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\naxes[0].imshow(np.array(noisy_digit).reshape(28, 28), cmap='binary')\naxes[0].set_title('Noisy Input', fontsize=14, fontweight='bold')\naxes[0].axis('off')\n\naxes[1].imshow(np.array(clean_digit).reshape(28, 28), cmap='binary')\naxes[1].set_title('Clean Target', fontsize=14, fontweight='bold')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train KNN for denoising\nprint(\"\\nTraining KNN for image denoising (may take a while)...\")\nknn_clf.fit(X_train_mod[:5000], y_train_mod[:5000])  # Use subset for speed\nprint(\"Model trained!\")\n\n# Denoise the test image\nclean_digit_pred = knn_clf.predict([noisy_digit])\n\n# Visualize results\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\naxes[0].imshow(np.array(noisy_digit).reshape(28, 28), cmap='binary')\naxes[0].set_title('Noisy Input', fontsize=12, fontweight='bold')\naxes[0].axis('off')\n\naxes[1].imshow(clean_digit_pred.reshape(28, 28), cmap='binary')\naxes[1].set_title('Denoised Output', fontsize=12, fontweight='bold')\naxes[1].axis('off')\n\naxes[2].imshow(np.array(clean_digit).reshape(28, 28), cmap='binary')\naxes[2].set_title('Original Target', fontsize=12, fontweight='bold')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nImage successfully denoised!\")\nprint(\"This demonstrates multioutput classification\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kesimpulan Chapter 3\n",
        "\n",
        "### Summary Metrics & Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\nprint(\"CHAPTER 3 SUMMARY\")\nprint(\"=\"*80)\n\nsummary = \"\"\"\nClassification Types Covered:\n  1. Binary Classification (5-detector)\n     • SGDClassifier\n     • RandomForestClassifier\n  \n  2. Multiclass Classification (0-9 digits)\n     • OvR (One-vs-Rest) strategy\n     • OvO (One-vs-One) strategy\n  \n  3. Multilabel Classification\n     • Multiple binary tags per instance\n     • KNeighborsClassifier\n  \n  4. Multioutput Classification\n     • Multiple multiclass labels per instance\n     • Image denoising example\n\nPerformance Metrics:\n  Accuracy           - Overall correctness (misleading for imbalanced data)\n  Confusion Matrix   - Detailed breakdown of predictions\n  Precision          - Accuracy of positive predictions\n  Recall (TPR)       - Coverage of actual positives\n  F₁ Score           - Harmonic mean of Precision & Recall\n  ROC Curve          - TPR vs FPR trade-off\n  ROC AUC            - Area under ROC curve\n\nKey Techniques:\n  • Cross-validation for robust evaluation\n  • Threshold tuning for Precision/Recall trade-off\n  • Confusion matrix analysis for error patterns\n  • Feature scaling for better performance\n  • Error analysis for model improvement\n\nBest Practices:\n  1. Never use accuracy alone for imbalanced datasets\n  2. Choose metric based on business requirements\n  3. Use confusion matrix to understand errors\n  4. Tune threshold based on Precision/Recall needs\n  5. Compare models using ROC curves and AUC\n  6. Analyze specific errors to improve model\n  7. Consider feature scaling (especially for SGD)\n\"\"\"\n\nprint(summary)\nprint(\"=\"*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### 1. **Metric Selection Guide**\n",
        "\n",
        "| Scenario | Recommended Metric |\n",
        "|----------|-------------------|\n",
        "| Balanced classes | **Accuracy** |\n",
        "| Imbalanced classes | **Precision, Recall, F₁** |\n",
        "| False Positive costly | **Precision** |\n",
        "| False Negative costly | **Recall** |\n",
        "| Need balance | **F₁ Score** |\n",
        "| Compare multiple models | **ROC AUC** |\n",
        "| Rare positive class | **PR Curve** |\n",
        "\n",
        "### 2. **When to Use Each Classifier**\n",
        "\n",
        "```python\n\"\"\"\nSGDClassifier:\n  Large datasets\n  Online learning\n  Fast training\n  Needs feature scaling\n  Sensitive to hyperparameters\n\nRandomForestClassifier:\n  Works well out-of-the-box\n  Handles missing values\n  Feature importance\n  Slower on large datasets\n  Memory intensive\n\nSVC (Support Vector Machine):\n  Effective in high dimensions\n  Memory efficient\n  Slow for large datasets\n  Requires feature scaling\n\nKNeighborsClassifier:\n  Simple and intuitive\n  Multilabel support\n  Slow prediction\n  Needs feature scaling\n\"\"\"\n```\n",
        "\n",
        "### 3. **Precision/Recall Decision Guide**\n",
        "\n",
        "```\nHigh Precision Needed:\n├─ Video safety classifier\n├─ Spam filter (avoid blocking ham)\n└─ Medical diagnosis (avoid false alarms)\n\nHigh Recall Needed:\n├─ Cancer screening\n├─ Fraud detection\n└─ Shoplifter detection\n```\n",
        "\n",
        "### 4. **Error Analysis Workflow**\n",
        "\n",
        "```\n1. Compute confusion matrix\n2. Identify most common errors\n3. Analyze misclassified examples\n4. Hypothesize causes:\n   ├─ Poor data quality?\n   ├─ Similar-looking classes?\n   ├─ Model too simple?\n   └─ Features inadequate?\n5. Take action:\n   ├─ Gather more data\n   ├─ Engineer new features\n   ├─ Preprocess images\n   ├─ Try different model\n   └─ Adjust hyperparameters\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises (from the book)\n",
        "\n",
        "### Exercise 1: Build 97%+ Accuracy Classifier\n",
        "\n",
        "```python\n\"\"\"\nTask: Achieve >97% accuracy on MNIST test set\n\nHints:\n- Use KNeighborsClassifier\n- Grid search on:\n  • weights: ['uniform', 'distance']\n  • n_neighbors: [3, 4, 5]\n- Don't forget feature scaling!\n\nYour code here...\n\"\"\"\n```\n",
        "\n",
        "### Exercise 2: Data Augmentation\n",
        "\n",
        "```python\n\"\"\"\nTask: Shift MNIST images and augment training set\n\nSteps:\n1. Write function to shift image by 1 pixel (left/right/up/down)\n2. For each training image, create 4 shifted copies\n3. Add to training set (60k → 300k images)\n4. Train best model\n5. Measure test accuracy\n\nHint: Use scipy.ndimage.interpolation.shift()\n\nYour code here...\n\"\"\"\n```\n",
        "\n",
        "### Exercise 3: Titanic Dataset\n",
        "\n",
        "```python\n\"\"\"\nTask: Build classifier for Titanic survival prediction\n\nDataset: https://www.kaggle.com/c/titanic\n\nSteps:\n1. Load data from Kaggle\n2. Explore and visualize\n3. Handle missing values\n4. Feature engineering\n5. Try multiple classifiers\n6. Evaluate and compare\n\nYour code here...\n\"\"\"\n```\n",
        "\n",
        "### Exercise 4: Spam Classifier\n",
        "\n",
        "```python\n\"\"\"\nTask: Build spam classifier with high precision & recall\n\nDataset: Apache SpamAssassin public datasets\n\nSteps:\n1. Download spam and ham examples\n2. Explore data format\n3. Split train/test sets\n4. Build preprocessing pipeline:\n   - Remove headers (optional)\n   - Convert to lowercase\n   - Remove punctuation\n   - Replace URLs with \"URL\"\n   - Replace numbers with \"NUMBER\"\n   - Stemming (optional)\n5. Convert to feature vectors:\n   - Option A: Binary (word present/absent)\n   - Option B: Count (word frequency)\n6. Try multiple classifiers\n7. Evaluate with Precision & Recall\n\nYour code here...\n\"\"\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Concepts\n",
        "\n",
        "### Cross-Validation Explained\n",
        "\n",
        "```python\n\"\"\"\nStratifiedKFold (n_splits=3):\n\nOriginal: [A][A][A][B][B][B][C][C][C]\n\nFold 1:\n  Train: [A][A][B][B][C][C]\n  Val:   [A][B][C]\n\nFold 2:\n  Train: [A][B][B][C][C][A]\n  Val:   [A][B][C]\n\nFold 3:\n  Train: [A][B][C][B][C][A]\n  Val:   [A][B][C]\n\nEach fold maintains class distribution!\n\"\"\"\n```\n",
        "\n",
        "### One-Hot Encoding Visualization\n",
        "\n",
        "```python\n\"\"\"\nBefore:\n  ocean_proximity = ['INLAND', '<1H OCEAN', 'ISLAND']\n\nAfter:\n                 INLAND  <1H_OCEAN  NEAR_BAY  NEAR_OCEAN  ISLAND\n  Instance 1:      1         0          0          0         0\n  Instance 2:      0         1          0          0         0\n  Instance 3:      0         0          0          0         1\n\nEach category becomes a binary feature!\n\"\"\"\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "**Scikit-Learn Documentation:**\n",
        "- Classification metrics: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "- Classifiers: https://scikit-learn.org/stable/supervised_learning.html\n",
        "\n",
        "**Papers:**\n",
        "- Precision-Recall curves (Davis & Goadrich, 2006)\n",
        "- ROC analysis (Fawcett, 2006)\n",
        "\n",
        "**Books:**\n",
        "- \"Pattern Recognition and Machine Learning\" - Bishop\n",
        "- \"The Elements of Statistical Learning\" - Hastie et al.\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Classifying!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}