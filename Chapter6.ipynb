{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 6: Decision Trees\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“– Rangkuman Chapter 6\n",
        "\n",
        "Chapter ini membahas **Decision Trees**, salah satu algoritme Machine Learning yang paling serbaguna. Seperti SVM, Decision Trees dapat melakukan tugas klasifikasi, regresi, dan bahkan multioutput.\n",
        "\n",
        "Mereka adalah algoritme yang kuat, mampu menangani dataset yang kompleks, dan merupakan komponen dasar dari **Random Forests**, salah satu model ML paling kuat yang ada saat ini. Chapter ini akan mencakup cara melatih, memvisualisasikan, dan membuat prediksi dengan Decision Trees, serta algoritme CART, regularisasi, dan keterbatasannya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Topics Covered\n",
        "\n",
        "| No | Topic | Description |\n",
        "|----|-------|-------------|\n",
        "| 1 | **Training & Visualizing** | Melatih `DecisionTreeClassifier` dan memvisualisasikannya |\n",
        "| 2 | **Making Predictions** | Cara kerja pohon dalam mengambil keputusan (root-to-leaf) |\n",
        "| 3 | **White Box vs. Black Box** | Mengapa Decision Trees mudah diinterpretasi |\n",
        "| 4 | **Estimating Probabilities** | Cara pohon menghasilkan probabilitas kelas |\n",
        "| 5 | **CART Algorithm** | Algoritme *greedy* yang digunakan Scikit-Learn untuk melatih pohon |\n",
        "| 6 | **Gini vs. Entropy** | Dua metode untuk mengukur \"impurity\" (kemurnian) node |\n",
        "| 7 | **Regularization** | Cara mencegah overfitting (misalnya, `max_depth`, `min_samples_leaf`) |\n",
        "| 8 | **Regression** | Menggunakan Decision Trees untuk tugas regresi (`DecisionTreeRegressor`) |\n",
        "| 9 | **Limitations** | Keterbatasan utama: sensitivitas terhadap rotasi dan variasi data |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŒ³ Training & Visualizing a Decision Tree\n",
        "\n",
        "Kita dapat melatih `DecisionTreeClassifier` pada dataset Iris hanya dengan beberapa baris kode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "# Melatih model dengan membatasi kedalaman maksimum (regularisasi)\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)\n",
        "\n",
        "print(\"Model Decision Tree telah dilatih!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setelah dilatih, pohon ini dapat divisualisasikan menggunakan `export_graphviz()` dari Scikit-Learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "    tree_clf,\n",
        "    out_file=\"iris_tree.dot\",\n",
        "    feature_names=iris.feature_names[2:],\n",
        "    class_names=iris.target_names,\n",
        "    rounded=True,\n",
        "    filled=True\n",
        ")\n",
        "\n",
        "print(\"File 'iris_tree.dot' telah dibuat.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(File `.dot` ini kemudian dapat dikonversi menjadi PNG menggunakan alat Graphviz).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§­ Making Predictions\n",
        "\n",
        "Proses prediksi sangat intuitif, mirip dengan permainan \"20 Questions\".\n",
        "\n",
        "1.  Anda mulai dari **root node** (node paling atas, depth 0). Node ini bertanya: \"Apakah petal length <= 2.45 cm?\"\n",
        "2.  Jika **True**, Anda pindah ke node anak kiri (depth 1, left). Node ini adalah **leaf node** (tidak punya anak). Pohon memprediksi `class=setosa`.\n",
        "3.  Jika **False** (petal length > 2.45 cm), Anda pindah ke node anak kanan (depth 1, right).\n",
        "4.  Node ini **bukan leaf node**, jadi ia bertanya lagi: \"Apakah petal width <= 1.75 cm?\"\n",
        "5.  Jika **True**, pohon memprediksi `class=versicolor` (depth 2, left).\n",
        "6.  Jika **False**, pohon memprediksi `class=virginica` (depth 2, right).\n",
        "\n",
        "ðŸ’¡ **Kualitas Utama:** Salah satu keunggulan Decision Trees adalah mereka **tidak memerlukan feature scaling atau centering** sama sekali.\n",
        "\n",
        "### Atribut Node\n",
        "\n",
        "* `samples`: Menghitung berapa banyak instance pelatihan yang \"melewati\" node tersebut.\n",
        "* `value`: Menunjukkan berapa banyak instance pelatihan dari setiap kelas yang ada di node ini.\n",
        "* `gini`: Mengukur **impurity** (ketidakmurnian) node.\n",
        "    * Node \"murni\" ($gini=0$) jika semua instance di dalamnya milik satu kelas yang sama.\n",
        "\n",
        "### Gini Impurity\n",
        "\n",
        "Skor Gini dihitung menggunakan formula berikut:\n",
        "$$G_{i}=1-\\sum_{k=1}^{n}{p_{i,k}}^{2}$$\n",
        "* $p_{i,k}$ adalah rasio (proporsi) instance kelas $k$ di antara total instance di node $i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ White Box vs. Black Box\n",
        "\n",
        "Decision Trees dianggap sebagai model **\"white box\"**.\n",
        "* âœ… **White Box:** Model ini intuitif, dan keputusan yang diambilnya mudah untuk diinterpretasi. Anda dapat dengan jelas melihat aturan-aturan yang digunakan untuk membuat prediksi (misalnya, \"JIKA petal_length <= 2.45 MAKA setosa\").\n",
        "\n",
        "Sebaliknya, Random Forests atau Neural Networks dianggap sebagai model **\"black box\"**.\n",
        "* âŒ **Black Box:** Meskipun prediksinya hebat, biasanya sangat sulit untuk menjelaskan *mengapa* model tersebut membuat prediksi tertentu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Estimating Class Probabilities\n",
        "\n",
        "Decision Tree juga dapat memperkirakan probabilitas sebuah instance milik kelas $k$.\n",
        "\n",
        "1.  Pertama, pohon menelusuri jalurnya untuk menemukan leaf node untuk instance tersebut.\n",
        "2.  Kemudian, ia mengembalikan **rasio instance pelatihan** dari kelas $k$ di leaf node itu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§® CART Algorithm\n",
        "\n",
        "Scikit-Learn menggunakan algoritme **Classification and Regression Tree (CART)** untuk melatih Decision Trees.\n",
        "\n",
        "Ide dasar CART adalah algoritme *greedy*: Ia mencari split optimal pada level atas, kemudian mengulang proses ini pada level berikutnya. (Tidak memeriksa apakah split akan menghasilkan biaya terendah secara keseluruhanâ€”ini adalah pendekatan \"greedy\" yang tidak menjamin solusi optimal, tetapi cukup baik dan sederhana.)\n",
        "\n",
        "**CART Cost Function (Classification):**\n",
        "$$J(k,t_{k})=\\frac{m_{left}}{m}G_{left}+\\frac{m_{right}}{m}G_{right}$$\n",
        "* $G_{left/right}$ = Gini impurity dari subset kiri/kanan.\n",
        "* $m_{left/right}$ = Jumlah instance di subset kiri/kanan.\n",
        "\n",
        "**Complexity:**\n",
        "* [cite_start]Membuat prediksi: $O(\\log_2(m))$ (balanced tree). [cite: 854]\n",
        "* [cite_start]Pelatihan: $O(n \\times m \\log_2(m))$ (n = fitur, m = instance). [cite: 801]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gini vs. Entropy\n",
        "\n",
        "Scikit-Learn menggunakan **Gini impurity** secara default (`criterion=\"gini\"`), tapi Anda bisa mengubahnya menjadi **entropy** (`criterion=\"entropy\"`).\n",
        "\n",
        "**Entropy Impurity:**\n",
        "$$H_{i}=-\\sum_{k=1}^{n}p_{i,k}\\log_{2}(p_{i,k})$$\n",
        "* Node dengan entropy=0 adalah \"murni\" (semua instance sama kelas).\n",
        "* Hampir sama dengan Gini, tapi menghasilkan pohon yang sedikit lebih seimbang.\n",
        "\n",
        "**Gini vs. Entropy:**\n",
        "* Gini lebih cepat dihitung (tidak ada log).\n",
        "* Perbedaan praktisnya kecil; keduanya menghasilkan pohon yang mirip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization\n",
        "\n",
        "Decision Trees membuat sangat sedikit asumsi tentang data (non-parametric), sehingga jika tidak dibatasi, model cenderung **overfit**.\n",
        "\n",
        "**Hyperparameters Regularization:**\n",
        "* `max_depth`: Batas kedalaman pohon.\n",
        "* `min_samples_split`: Minimum sampel untuk split node.\n",
        "* `min_samples_leaf`: Minimum sampel di leaf node.\n",
        "* `max_leaf_nodes`: Batas jumlah leaf nodes.\n",
        "* `max_features`: Jumlah fitur yang dievaluasi untuk setiap split.\n",
        "\n",
        "Meningkatkan `min_*` atau mengurangi `max_*` akan meregularisasi model (mengurangi overfitting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression\n",
        "\n",
        "Decision Trees juga bisa digunakan untuk regresi (`DecisionTreeRegressor`).\n",
        "\n",
        "Prediksi: Alih-alih memprediksi kelas, regresi memprediksi **nilai rata-rata** dari instance di leaf node.\n",
        "\n",
        "**CART for Regression:**\n",
        "Algoritme sama, tetapi alih-alih meminimalkan *impurity* (Gini/Entropy), ia mencoba membagi data untuk meminimalkan **MSE (Mean Squared Error)**.\n",
        "\n",
        "**CART Cost Function (Regression):**\n",
        "$$J(k,t_{k})=\\frac{m_{left}}{m}MSE_{left}+\\frac{m_{right}}{m}MSE_{right}$$\n",
        "* Di mana $MSE_{node} = \\frac{1}{m_{node}} \\sum_{i \\in \\text{node}} (\\hat{y}_{node} - y^{(i)})^2$\n",
        "\n",
        "Sama seperti klasifikasi, Decision Tree Regressor juga sangat rentan terhadap **overfitting** jika tidak diregularisasi (misalnya, dengan `min_samples_leaf=10`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‰ Instability & Limitations\n",
        "\n",
        "Meskipun kuat, Decision Trees memiliki beberapa kelemahan:\n",
        "\n",
        "1.  **Sensitivitas terhadap Rotasi Data**\n",
        "    * Decision Trees menyukai batas keputusan yang **ortogonal** (tegak lurus dengan sumbu fitur).\n",
        "    * Ini membuat mereka sangat sensitif terhadap rotasi data. Jika data diputar (misalnya 45 derajat), pohon akan membuat batas keputusan \"tangga\" yang rumit dan tidak akan menggeneralisasi dengan baik.\n",
        "\n",
        "2.  **Sensitivitas terhadap Variasi Data (Instabilitas)**\n",
        "    * Isu utama Decision Trees adalah mereka **sangat sensitif terhadap variasi kecil** dalam data pelatihan.\n",
        "    * Misalnya, jika Anda hanya menghapus satu instance (bunga Iris versicolor terlebar) dari dataset, Anda mungkin mendapatkan pohon yang terlihat sangat berbeda.\n",
        "    * Algoritme Scikit-Learn juga bersifat *stochastic* (acak), sehingga Anda bisa mendapatkan model yang berbeda bahkan pada data yang sama (kecuali Anda mengatur `random_state`).\n",
        "\n",
        "ðŸ’¡ **Solusi:** **Random Forests** (dibahas di Bab 7) dapat mengatasi instabilitas ini dengan mengambil rata-rata prediksi dari banyak pohon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Exercises (from the book)\n",
        "\n",
        "### Exercise 1\n",
        "**Q:** What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?\n",
        "**A:** $O(\\log_2(m))$. Untuk $m = 1.000.000$, $\\log_2(10^6) \\approx 19.93$. Jadi, kedalamannya sekitar **20**.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2\n",
        "**Q:** Is a node's Gini impurity generally lower or greater than its parent's? Is it generally lower/greater, or *always* lower/greater?\n",
        "**A:** **Generally lower**. Algoritme CART (Eq. 6-2) secara eksplisit mencari split yang meminimalkan *weighted average* Gini impurity dari anak-anaknya. Karena itu, *weighted average* Gini *selalu* lebih rendah. Namun, Gini satu *anak* (child) bisa saja lebih tinggi dari induknya, asalkan *total weighted average*-nya lebih rendah.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3\n",
        "**Q:** If a Decision Tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
        "**A:** **Ya**. Mengurangi `max_depth` adalah salah satu cara utama untuk meregularisasi model dan mengurangi overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4\n",
        "**Q:** If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
        "**A:** **Tidak**. Decision Trees tidak sensitif terhadap penskalaan fitur, jadi itu tidak akan membantu. Jika underfitting, Anda harus *meningkatkan* kompleksitas (misalnya, *meningkatkan* `max_depth` atau *mengurangi* `min_samples_leaf`).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5\n",
        "**Q:** If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?\n",
        "**A:** Kompleksitas pelatihan adalah $O(n \\times m \\log_2(m))$. \n",
        "Rasio waktu $\\approx \\frac{10M \\log_2(10M)}{1M \\log_2(1M)} \\approx 10 \\times \\frac{23.3}{19.9} \\approx 11.7$.\n",
        "Jadi, akan memakan waktu sekitar **11.7 jam**.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6\n",
        "**Q:** If your training set contains 100,000 instances, will setting `presort=True` speed up training?\n",
        "**A:** **Tidak**. Presorting hanya mempercepat pelatihan pada dataset kecil (kurang dari beberapa ribu instance) dan akan **memperlambat** pelatihan secara signifikan pada dataset yang lebih besar.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 7\n",
        "**Q:** Train and fine-tune a Decision Tree for the moons dataset...\n",
        "**A:** (Requires implementation)\n",
        "* `make_moons(n_samples=10000, noise=0.4)`\n",
        "* `train_test_split()`\n",
        "* `GridSearchCV` untuk `DecisionTreeClassifier`, coba berbagai nilai `max_leaf_nodes`.\n",
        "* Latih pada set penuh dan ukur performa (target ~85-87% akurasi).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8\n",
        "**Q:** Grow a forest...\n",
        "**A:** (Requires implementation)\n",
        "* Buat 1.000 subset (masing-masing 100 instance) dari training set (gunakan `ShuffleSplit`).\n",
        "* Latih 1.000 Decision Trees menggunakan hyperparameter terbaik dari Ex. 7.\n",
        "* Evaluasi 1.000 pohon di test set (performa mungkin turun).\n",
        "* Untuk setiap instance test set, ambil prediksi mayoritas (majority vote) dari 1.000 pohon (gunakan `scipy.stats.mode()`).\n",
        "* Evaluasi prediksi *majority-vote* ini. Akurasi seharusnya meningkat sedikit (0.5-1.5%) dibanding model pertama.\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Splitting! ðŸŒ³**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}