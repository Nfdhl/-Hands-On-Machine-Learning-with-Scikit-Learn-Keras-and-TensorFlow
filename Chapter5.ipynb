{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5: Support Vector Machines\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Rangkuman Chapter 5\n",
        "\n",
        "Chapter ini memperkenalkan **Support Vector Machine (SVM)**, sebuah model yang sangat kuat dan serbaguna. SVM mampu melakukan klasifikasi (linier dan nonlinier), regresi, dan bahkan deteksi outlier.\n",
        "\n",
        "Model ini adalah salah satu yang paling populer di Machine Learning dan sangat cocok untuk dataset berukuran kecil hingga menengah yang kompleks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Topics Covered\n",
        "\n",
        "| No | Topic | Description |\n",
        "|----|-------|-------------|\n",
        "| 1 | **Linear SVM** | Ide dasar \"Large Margin Classification\"  |\n",
        "| 2 | **Hard vs. Soft Margin** | `C` hyperparameter & margin violations  |\n",
        "| 3 | **Nonlinear SVM** | Mengatasi data yang tidak terpisah linier  |\n",
        "| 4 | **Polynomial Features** | Pendekatan manual untuk data nonlinier  |\n",
        "| 5 | **Kernel Trick** | \"Keajaiban\" SVM: Poly & RBF Kernel  |\n",
        "| 6 | **Tuning Hyperparameters** | Peran `C` dan `gamma`  |\n",
        "| 7 | **SVM Regression** | Menggunakan SVM untuk prediksi nilai  |\n",
        "| 8 | **Computational Complexity** | `LinearSVC` vs `SVC`  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Linear SVM Classification\n",
        "\n",
        "### Fundamental Idea: Large Margin\n",
        "\n",
        "Ide fundamental SVM adalah **\"large margin classification\"**. Tujuannya bukan hanya untuk memisahkan kelas, tetapi untuk menemukan batas keputusan yang memiliki \"jalan\" (margin) terluas di antara kelas-kelas tersebut.\n",
        "\n",
        "\n",
        "### Support Vectors\n",
        "\n",
        "**Support Vectors** adalah instance (titik data) yang terletak tepat di tepi \"jalan\" (margin). Mereka adalah satu-satunya titik data yang \"mendukung\" atau menentukan batas keputusan. Menambahkan lebih banyak data *di luar* jalan tidak akan memengaruhi batas keputusan sama sekali.\n",
        "\n",
        "### âš ï¸ Sensitivity to Feature Scales\n",
        "\n",
        "SVM sangat **sensitif terhadap skala fitur**. Jika fitur-fitur memiliki skala yang sangat berbeda (misalnya, satu dari 0-1 dan lainnya dari 0-1000), SVM akan kesulitan menemukan margin terbesar yang seimbang.\n",
        "\n",
        "**Penting:** Selalu lakukan penskalaan fitur (misalnya, `StandardScaler`) sebelum melatih SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš–ï¸ Hard Margin vs. Soft Margin\n",
        "\n",
        "### Hard Margin Classification\n",
        "\n",
        "Model ini secara ketat mengharuskan *semua* instance berada di luar jalan dan di sisi yang benar.\n",
        "\n",
        "**Cons:**\n",
        "* âŒ Hanya berfungsi jika data **benar-benar dapat dipisahkan secara linier**.\n",
        "* âŒ Sangat **sensitif terhadap outlier**. Satu outlier saja bisa membuat model gagal menemukan margin atau menghasilkan margin yang sangat buruk.\n",
        "\n",
        "### Soft Margin Classification\n",
        "\n",
        "Ini adalah model yang lebih fleksibel dan umum digunakan. Tujuannya adalah menemukan keseimbangan antara dua hal:\n",
        "1.  Menjaga \"jalan\" (margin) selebar mungkin.\n",
        "2.  Membatasi **\"margin violations\"** (pelanggaran margin)â€”yaitu, instance yang berada di tengah jalan atau bahkan di sisi yang salah.\n",
        "\n",
        "**Hyperparameter `C`:**\n",
        "Di Scikit-Learn, hyperparameter `C` mengontrol keseimbangan ini.\n",
        "* **Nilai `C` rendah**: Margin lebih lebar, tetapi lebih banyak pelanggaran margin (Regularisasi lebih tinggi). Ini bagus jika model Anda **overfitting**.\n",
        "* **Nilai `C` tinggi**: Margin lebih sempit, tetapi lebih sedikit pelanggaran margin (Regularisasi lebih rendah). Model mencoba untuk mengklasifikasikan setiap instance dengan benar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŒ€ Nonlinear SVM Classification\n",
        "\n",
        "Banyak dataset di dunia nyata tidak dapat dipisahkan secara linier.\n",
        "\n",
        "### Approach 1: Polynomial Features\n",
        "\n",
        "Salah satu cara untuk menangani data nonlinier adalah dengan **menambahkan lebih banyak fitur**, seperti fitur polinomial.\n",
        "\n",
        "Sebagai contoh, dataset 1D yang tidak terpisah linier bisa menjadi terpisah linier jika Anda menambahkan fitur kedua \\(x_2 = (x_1)^2\\). Setelah transformasi ini, Anda dapat menggunakan `LinearSVC` biasa.\n",
        "\n",
        "**Cons:**\n",
        "* âŒ Derajat polinomial rendah tidak bisa menangani dataset yang sangat kompleks.\n",
        "* âŒ Derajat polinomial tinggi menciptakan **ledakan jumlah fitur** (combinatorial explosion) dan membuat model sangat lambat.\n",
        "\n",
        "### Approach 2: The Kernel Trick\n",
        "\n",
        "Di sinilah keajaiban SVM terjadi. **Kernel trick** adalah teknik matematis yang memungkinkan Anda mendapatkan hasil yang sama persis seolah-olah Anda telah menambahkan *banyak* fitur polinomial (bahkan derajat sangat tinggi) tanpa *benar-benar* menambahkannya.\n",
        "\n",
        "Tidak ada ledakan jumlah fitur karena Anda tidak benar-benar membuat fitur baru. Ini diimplementasikan oleh kelas `SVC` (Support Vector Classifier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ¨ SVM Kernels\n",
        "\n",
        "Kernel adalah fungsi yang mampu menghitung dot product dari vektor yang ditransformasi, hanya dengan menggunakan vektor asli.\n",
        "\n",
        "### Polynomial Kernel\n",
        "\n",
        "Mensimulasikan penambahan fitur polinomial.\n",
        "\n",
        "**Equation:**\n",
        "$$K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^T \\mathbf{b} + r)^d$$\n",
        "\n",
        "**Hyperparameters:**\n",
        "* `degree` (\\(d\\)): Mengontrol derajat polinomial. Jika model overfitting, kurangi nilainya. Jika underfitting, tingkatkan.\n",
        "* `coef0` (\\(r\\)): Mengontrol seberapa besar model dipengaruhi oleh polinomial derajat tinggi vs. derajat rendah.\n",
        "\n",
        "### Gaussian RBF Kernel\n",
        "\n",
        "Mensimulasikan penambahan \"fitur kesamaan\" (similarity features). Bekerja dengan mengukur seberapa \"mirip\" sebuah instance dengan \"landmark\" tertentu.\n",
        "\n",
        "**Equation:**\n",
        "$$K(\\mathbf{a}, \\mathbf{b}) = \\exp(-\\gamma ||\\mathbf{a} - \\mathbf{b}||^2)$$\n",
        "\n",
        "**Hyperparameter `gamma` (\\(\\gamma\\)):**\n",
        "`gamma` bertindak sebagai hyperparameter regularisasi.\n",
        "* **Nilai \\(\\gamma\\) kecil**: Kurva RBF (lonceng) menjadi lebar. Instance memiliki jangkauan pengaruh yang besar, membuat batas keputusan lebih **halus**. (Risiko **Underfitting**)\n",
        "* **Nilai \\(\\gamma\\) besar**: Kurva RBF (lonceng) menjadi sempit. Jangkauan pengaruh tiap instance kecil, membuat batas keputusan lebih **tidak teratur** dan \"bergoyang\" menurut instance. (Risiko **Overfitting**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’» Kode dan Eksekusi\n",
        "\n",
        "### 1. Klasifikasi SVM Linier (Dataset Iris)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Muat dataset\n",
        "iris = datasets.load_iris()\n",
        "# Ambil fitur petal length dan petal width\n",
        "X = iris[\"data\"][:, (2, 3)]  \n",
        "# Konversi target menjadi biner (1 jika Iris virginica, 0 jika tidak)\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica \n",
        "\n",
        "# Buat pipeline: 1. Scaling, 2. Model Linear SVM\n",
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")) \n",
        "])\n",
        "\n",
        "# Latih model\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "print(\"Model LinearSVC telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi\n",
        "prediksi = svm_clf.predict([[5.5, 1.7]])\n",
        "print(f\"Prediksi untuk petal length=5.5, petal width=1.7: {prediksi}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Klasifikasi SVM Nonlinier (Fitur Polinomial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Buat dataset moons\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "# Buat pipeline: 1. Fitur Polinomial (derajat 3), 2. Scaling, 3. Model Linear SVM\n",
        "polynomial_svm_clf = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", max_iter=10000))\n",
        "])\n",
        "\n",
        "# Latih model\n",
        "polynomial_svm_clf.fit(X, y)\n",
        "\n",
        "print(\"Model SVM Polinomial (manual) telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi (contoh)\n",
        "pred_poly_manual = polynomial_svm_clf.predict([[1.0, 0.5]])\n",
        "print(f\"Prediksi untuk [1.0, 0.5]: {pred_poly_manual}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Klasifikasi SVM (Polynomial Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Buat pipeline: 1. Scaling, 2. Model SVC dengan kernel polinomial\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])\n",
        "\n",
        "# Latih model\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "print(\"Model SVM (Polynomial Kernel) telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi (contoh)\n",
        "pred_poly_kernel = poly_kernel_svm_clf.predict([[1.0, 0.5]])\n",
        "print(f\"Prediksi untuk [1.0, 0.5]: {pred_poly_kernel}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Klasifikasi SVM (Gaussian RBF Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Buat pipeline: 1. Scaling, 2. Model SVC dengan kernel RBF\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "])\n",
        "\n",
        "# Latih model\n",
        "rbf_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "print(\"Model SVM (RBF Kernel) telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi (contoh)\n",
        "pred_rbf_kernel = rbf_kernel_svm_clf.predict([[1.0, 0.5]])\n",
        "print(f\"Prediksi untuk [1.0, 0.5]: {pred_rbf_kernel}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Regresi SVM Linier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "# Buat data linier acak\n",
        "np.random.seed(42)\n",
        "X_reg = 2 * np.random.rand(50, 1)\n",
        "y_reg = 4 + 3 * X_reg[:, 0] + np.random.randn(50)\n",
        "X_reg = X_reg.reshape(-1, 1) # Reshape y\n",
        "y_reg = y_reg.reshape(-1)    # Reshape y\n",
        "\n",
        "# Inisialisasi model\n",
        "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
        "\n",
        "# Latih model (Scaling direkomendasikan, tapi diabaikan untuk kesederhanaan contoh)\n",
        "svm_reg.fit(X_reg, y_reg)\n",
        "\n",
        "print(\"Model LinearSVR telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi (contoh)\n",
        "pred_svr_linear = svm_reg.predict([[1.5]])\n",
        "print(f\"Prediksi untuk X=1.5: {pred_svr_linear}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Regresi SVM Nonlinier (Polynomial Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# Buat data kuadratik acak\n",
        "np.random.seed(42)\n",
        "m = 100\n",
        "X_quad = 2 * np.random.rand(m, 1) - 1\n",
        "y_quad = 0.2 + 0.1 * X_quad[:, 0] + 0.5 * X_quad[:, 0]**2 + np.random.randn(m) / 10\n",
        "X_quad = X_quad.reshape(-1, 1) # Reshape X\n",
        "y_quad = y_quad.reshape(-1)    # Reshape y\n",
        "\n",
        "# Inisialisasi model\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "\n",
        "# Latih model (Scaling direkomendasikan, tapi diabaikan untuk kesederhanaan contoh)\n",
        "svm_poly_reg.fit(X_quad, y_quad)\n",
        "\n",
        "print(\"Model SVR (Polynomial Kernel) telah dilatih.\")\n",
        "\n",
        "# Lakukan prediksi (contoh)\n",
        "pred_svr_poly = svm_poly_reg.predict([[0.5]])\n",
        "print(f\"Prediksi untuk X=0.5: {pred_svr_poly}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Kesimpulan Chapter 5\n",
        "\n",
        "### ğŸ“Š Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CHAPTER 5 SUMMARY - SVM CLASSES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_table = \"\"\"\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Class           â”‚ Use Case             â”‚ Key Characteristics       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ LinearSVC       â”‚ Linear SVM Classif.  â”‚ â€¢ Basis: liblinear (Cepat)â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Tidak support kernel    â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Skala O(m x n)          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ SVC             â”‚ Nonlinear SVM Classifâ”‚ â€¢ Basis: libsvm           â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Support kernel trick    â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Lambat, O(mÂ³) [Nonlin]  â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Baik untuk data < 100k  â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ LinearSVR       â”‚ Linear SVM Regress.  â”‚ â€¢ Mirip LinearSVC         â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Cepat, skala O(m x n)   â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ SVR             â”‚ Nonlinear SVM Regressâ”‚ â€¢ Mirip SVC               â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Support kernel trick    â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Lambat untuk data besar â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ SGDClassifier   â”‚ Linear SVM (Online)  â”‚ â€¢ loss='hinge'            â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Skala O(m x n)          â”‚\n",
        "â”‚                 â”‚                      â”‚ â€¢ Support out-of-core     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\"\"\"\n",
        "\n",
        "print(summary_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ Key Takeaways\n",
        "\n",
        "**1. SVM = Large Margin:**\n",
        "- Ide intinya adalah mencari margin terluas.\n",
        "- Hanya *support vectors* (titik di tepi) yang penting.\n",
        "\n",
        "**2. Scaling is Mandatory:**\n",
        "- SVM sensitif terhadap skala fitur. Selalu gunakan `StandardScaler`.\n",
        "\n",
        "**3. `C` Hyperparameter (Soft Margin):**\n",
        "- Mengontrol trade-off antara margin vs. violations.\n",
        "- `C` rendah = Regularisasi tinggi (margin lebar, lebih toleran).\n",
        "- `C` tinggi = Regularisasi rendah (margin sempit, kurang toleran).\n",
        "\n",
        "**4. Kernel Trick (Poly, RBF):**\n",
        "- Cara efisien untuk menangani data nonlinier tanpa membuat fitur baru.\n",
        "- **RBF** adalah kernel default yang paling sering bekerja.\n",
        "\n",
        "**5. `gamma` Hyperparameter (RBF Kernel):**\n",
        "- Mengontrol \"jangkauan\" tiap instance.\n",
        "- `gamma` rendah = Regularisasi tinggi (batas halus, risiko underfit).\n",
        "- `gamma` tinggi = Regularisasi rendah (batas bergelombang, risiko overfit).\n",
        "\n",
        "**6. Complexity Matters:**\n",
        "- `LinearSVC` sangat cepat untuk data linier.\n",
        "- `SVC` sangat kuat untuk data nonlinier, tetapi lambat jika data besar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Exercises (from the book)\n",
        "\n",
        "### Exercise 1\n",
        "**Q:** What is the fundamental idea behind Support Vector Machines? \n",
        "**A:** Ide fundamentalnya adalah *large margin classification*. SVM mencari batas keputusan yang tidak hanya memisahkan kelas, tetapi juga berada **sejauh mungkin** dari instance pelatihan terdekat di kedua kelas.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2\n",
        "**Q:** What is a support vector? \n",
        "**A:** Support vector adalah instance pelatihan yang terletak di \"tepi jalan\" (margin). Batas keputusan *sepenuhnya* ditentukan oleh instance-instance ini.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3\n",
        "**Q:** Why is it important to scale the inputs when using SVMs? \n",
        "**A:** Karena SVM sensitif terhadap skala fitur. Jika fitur tidak diskalakan, SVM akan cenderung memberikan margin yang sempit dan tidak seimbang, karena fitur dengan skala besar akan mendominasi.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4\n",
        "**Q:** Can an SVM classifier output a confidence score? What about a probability? \n",
        "**A:**\n",
        "* **Confidence Score:** Ya. SVM menghitung skor fungsi keputusan (misal, \\(\\mathbf{w}^T \\mathbf{x} + b\\)). Semakin jauh skor ini dari 0, semakin \"percaya diri\" model tersebut.\n",
        "* **Probability:** Tidak secara langsung. SVM tidak menghasilkan probabilitas seperti Logistic Regression. Namun, Scikit-Learn `SVC` bisa dikalibrasi untuk menghasilkan probabilitas jika Anda mengatur `probability=True` saat inisialisasi (ini akan menjalankan kalibrasi Platt, yang memakan waktu).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5\n",
        "**Q:** Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features? \n",
        "**A:**\n",
        "* Ini adalah kasus \"dataset besar (\\(m\\) besar) dengan fitur yang tidak terlalu banyak (\\(n\\) kecil)\".\n",
        "* Bentuk *dual problem* akan lambat karena kompleksitasnya bergantung pada \\(m\\) (bisa \\(O(m^2)\\) atau \\(O(m^3)\\)).\n",
        "* Bentuk *primal problem* (yang dioptimalkan oleh `LinearSVC`) memiliki kompleksitas \\(O(m \\times n)\\).\n",
        "* **Jawaban:** Gunakan **primal form**. Ini jauh lebih cepat ketika jumlah instance (\\(m\\)) lebih besar dari jumlah fitur (\\(n\\)). (Catatan: Teks mengatakan *dual* lebih cepat jika \\(m < n\\), jadi kebalikannya, *primal* lebih cepat jika \\(m > n\\)).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6\n",
        "**Q:** Say you've trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease \\(\\gamma\\) (gamma)? What about `C`? \n",
        "**A:** Model tersebut **underfitting** (bias tinggi). Untuk meningkatkan kompleksitas/fleksibilitas model:\n",
        "* **`gamma` (\\(\\gamma\\)):** Anda harus **meningkatkannya**. `gamma` yang kecil bertindak sebagai regularizer kuat (batas halus). Meningkatkan `gamma` membuat batas lebih fleksibel (risiko overfit).\n",
        "* **`C`:** Anda harus **meningkatkannya**. `C` yang rendah adalah regularizer kuat (margin lebar). Meningkatkan `C` mengurangi regularisasi, memaksa model untuk lebih \"pas\" dengan data (mengurangi margin violations).\n",
        "\n",
        "---\n",
        "\n",
        "**Latihan 7-10**:\n",
        "* 7. Set parameter QP untuk soft margin...\n",
        "* 8. Latih `LinearSVC`, `SVC`, dan `SGDClassifier` pada dataset yang sama...\n",
        "* 9. Latih SVM classifier pada dataset MNIST...\n",
        "* 10. Latih SVM regressor pada dataset California housing...\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Kerneling! âœ¨ğŸŒ€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}