{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8: Dimensionality Reduction\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Rangkuman Chapter 8\n",
        "\n",
        "Chapter ini membahas **Dimensionality Reduction** (Pengurangan Dimensi), sebuah topik penting dalam Machine Learning. Banyak dataset di dunia nyata memiliki ribuan atau bahkan jutaan fitur (dimensi).\n",
        "\n",
        "Jumlah fitur yang sangat besar ini tidak hanya membuat pelatihan menjadi sangat lambat, tetapi juga dapat mempersulit pencarian solusi yang baik. Masalah ini dikenal sebagai **\"the curse of dimensionality\"** (kutukan dimensi).\n",
        "\n",
        "Untungnya, kita sering dapat mengurangi jumlah fitur secara signifikan. Chapter ini akan membahas \"kutukan\" tersebut dan dua pendekatan utama untuk mengatasinya (Proyeksi dan Manifold Learning), serta tiga teknik populer: **PCA**, **Kernel PCA**, dan **LLE**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Topics Covered\n",
        "\n",
        "| No | Topic | Description |\n",
        "|----|-------|-------------|\n",
        "| 1 | **The Curse of Dimensionality** | Mengapa data berdimensi tinggi itu aneh dan problematis |\n",
        "| 2 | **Main Approaches** | Perbedaan antara Proyeksi (Projection) dan Manifold Learning |\n",
        "| 3 | **PCA (Principal Component Analysis)** | Teknik reduksi dimensi paling populer (linier) |\n",
        "| 4 | **Explained Variance** | Cara mengukur seberapa banyak informasi yang dipertahankan oleh PCA |\n",
        "| 5 | **PCA Variants** | Randomized PCA (cepat) dan Incremental PCA (skala besar) |\n",
        "| 6 | **Kernel PCA (kPCA)** | PCA versi nonlinier menggunakan \"kernel trick\" |\n",
        "| 7 | **LLE (Locally Linear Embedding)** | Teknik Manifold Learning untuk \"membuka\" data yang terpilin |\n",
        "| 8 | **Other Techniques** | Pengenalan singkat MDS, Isomap, t-SNE, dan LDA |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåå The Curse of Dimensionality\n",
        "\n",
        "Kita terbiasa hidup di ruang 3D, sehingga intuisi kita gagal saat membayangkan ruang berdimensi tinggi.\n",
        "\n",
        "**Masalah Utama:**\n",
        "* **Data Menjadi Jarang (Sparse):** Di ruang berdimensi tinggi, \"ruang\" sangatlah besar. Dua titik acak dalam sebuah *unit square* (1x1) rata-rata berjarak 0.52, namun di *unit hypercube* 1.000.000 dimensi, jarak rata-ratanya sekitar 408.25!\n",
        "* **Semua Titik Ada di \"Tepi\":** Di hypercube 10.000 dimensi, probabilitas sebuah titik acak berada \"sangat dekat dengan batas\" adalah > 99.999999%.\n",
        "* **Risiko Overfitting:** Karena data sangat jarang, instance baru kemungkinan besar akan jauh dari instance pelatihan mana pun. Prediksi menjadi tidak dapat diandalkan (ekstrapolasi besar), yang meningkatkan risiko overfitting.\n",
        "* **Tidak Dapat Diatasi dengan Data:** Jumlah data yang diperlukan untuk menjaga kepadatan (density) data tumbuh secara **eksponensial** dengan jumlah dimensi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Main Approaches\n",
        "\n",
        "Ada dua pendekatan utama untuk mengurangi dimensi:\n",
        "\n",
        "| Approach | Description | Analogi | Kapan Digunakan |\n",
        "|---|---|---|---|\n",
        "| **Projection** | Memproyeksikan data ke *subspace* berdimensi lebih rendah (hyperplane). | Membuat bayangan 2D dari objek 3D di atas lantai. | Ketika data \"tergeletak\" di dekat subspace yang \"datar\" (linier). |\n",
        "| **Manifold Learning** | Mengasumsikan data terletak pada *manifold* (bentuk) berdimensi rendah yang terpilin atau tertekuk di dalam ruang berdimensi tinggi. | \"Membuka\" gulungan Swiss roll (kue gulung) untuk menjadikannya lembaran 2D datar. | Ketika data memiliki struktur nonlinier yang terpilin (seperti Swiss roll). |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è PCA (Principal Component Analysis)\n",
        "\n",
        "**PCA** adalah algoritme reduksi dimensi yang paling populer. PCA bekerja dengan cara **Proyeksi**.\n",
        "\n",
        "### Core Idea: Preserving the Variance\n",
        "\n",
        "Bagaimana PCA memilih hyperplane (subspace) terbaik untuk proyeksi?\n",
        "Ia memilih hyperplane yang **mempertahankan jumlah variance (varians) semaksimal mungkin**.\n",
        "\n",
        "\n",
        "Ini setara dengan memilih sumbu yang **meminimalkan mean squared distance** (jarak kuadrat rata-rata) antara data asli dan proyeksinya (ini disebut *reconstruction error*).\n",
        "\n",
        "### Principal Components (PCs)\n",
        "\n",
        "PCA menemukan sumbu-sumbu yang memaksimalkan varians:\n",
        "* **1st PC**: Sumbu yang menyimpan varians terbesar.\n",
        "* **2nd PC**: Sumbu kedua, yang **ortogonal (tegak lurus)** terhadap PC pertama, yang menyimpan varians tersisa terbesar.\n",
        "* Dan seterusnya.\n",
        "\n",
        "### Training PCA\n",
        "\n",
        "PCA menggunakan **Singular Value Decomposition (SVD)** untuk menemukan PCs.\n",
        "\n",
        "Di Scikit-Learn, cukup gunakan `PCA()` dan set `n_components` ke jumlah dimensi target (atau rasio varians yang ingin dipertahankan)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# PCA ke 2 dimensi\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)\n",
        "\n",
        "print(\"Shape asli:\", X.shape)\n",
        "print(\"Shape setelah PCA:\", X2D.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explained Variance\n",
        "\n",
        "Atribut `explained_variance_ratio_` memberi tahu seberapa banyak varians yang dipertahankan oleh masing-masing PC.\n",
        "\n",
        "Contoh pada Iris: PC pertama menyimpan 95.80% varians, PC kedua 3.36% (total 99.16%).\n",
        "\n",
        "Untuk memilih jumlah dimensi: Set `n_components` ke angka antara 0.0 dan 1.0 (rasio varians yang ingin dipertahankan, misalnya 0.95)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA Variants\n",
        "\n",
        "* **Randomized PCA**: Jika `n_components < 0.8 * n_features`, PCA menggunakan versi *randomized* yang lebih cepat.\n",
        "* **Incremental PCA (IPCA)**: Untuk dataset besar yang tidak muat di memori (out-of-core). Gunakan `partial_fit()` pada mini-batches.\n",
        "* **Kernel PCA (kPCA)**: Untuk data nonlinier (selanjutnya)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "import numpy as np\n",
        "\n",
        "# IPCA untuk dataset besar\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=2)\n",
        "for X_batch in np.array_split(X, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_reduced = inc_pca.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel PCA (kPCA)\n",
        "\n",
        "Kernel PCA adalah PCA versi nonlinier, menggunakan **kernel trick** (seperti SVM).\n",
        "\n",
        "* Bagus untuk data nonlinier (misalnya, Swiss roll).\n",
        "* Kernel populer: 'linear', 'rbf', 'sigmoid'.\n",
        "* **Hyperparameter Tuning**: Gunakan GridSearchCV dengan classifier downstream untuk memilih kernel dan hyperparameternya.\n",
        "* **Rekonstruksi Pre-image**: Anda bisa membalikkan reduksi dengan `inverse_transform()` (meskipun lossy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLE (Locally Linear Embedding)\n",
        "\n",
        "LLE adalah teknik **Manifold Learning** nonlinier.\n",
        "\n",
        "* **Cara Kerja**: Mengukur relasi linier lokal antar instance, lalu mencari representasi berdimensi rendah yang mempertahankan relasi itu semaksimal mungkin.\n",
        "* Bagus untuk membuka manifold yang terpilin (misalnya, Swiss roll).\n",
        "* **Complexity**: Pelatihan O(d m^2) untuk mencari k-neighbors, O(d n^3) untuk optimasi. Prediksi O(m)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other Techniques\n",
        "\n",
        "* **MDS (Multidimensional Scaling)**: Mirip PCA, tapi meminimalkan jarak antar instance (bukan varians).\n",
        "* **Isomap**: MDS versi graph-based (menghubungkan neighbors).\n",
        "* **t-SNE**: Untuk visualisasi (2D/3D), menjaga instance mirip tetap dekat.\n",
        "* **LDA (Linear Discriminant Analysis)**: Klasifikasi linier yang juga reduksi dimensi (proyeksi yang memisahkan kelas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Kesimpulan Chapter 8\n",
        "\n",
        "### üìä Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CHAPTER 8 SUMMARY - DIMENSIONALITY REDUCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_table = \"\"\"\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Technique            ‚îÇ Approach             ‚îÇ Best For                   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ PCA                  ‚îÇ Projection (Linear)  ‚îÇ General purpose            ‚îÇ\n",
        "‚îÇ                      ‚îÇ                      ‚îÇ Preserving variance        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Randomized PCA       ‚îÇ Projection           ‚îÇ Fast for large n_samples   ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Incremental PCA      ‚îÇ Projection           ‚îÇ Out-of-core / streaming    ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Kernel PCA           ‚îÇ Projection (Nonlin)  ‚îÇ Nonlinear data             ‚îÇ\n",
        "‚îÇ                      ‚îÇ Kernel trick         ‚îÇ Tunable kernels            ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ LLE                  ‚îÇ Manifold Learning    ‚îÇ Unrolling manifolds        ‚îÇ\n",
        "‚îÇ                      ‚îÇ (Nonlinear)          ‚îÇ Local linear relations     ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ MDS                  ‚îÇ Distance Preserv.    ‚îÇ Similar to PCA             ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Isomap               ‚îÇ Graph-based MDS      ‚îÇ Geodesic distances         ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ t-SNE                ‚îÇ Local Similarities   ‚îÇ Visualization (2D/3D)      ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ LDA                  ‚îÇ Supervised Proj.     ‚îÇ Classification + Dim Red.  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\"\"\"\n",
        "\n",
        "print(summary_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Key Takeaways\n",
        "\n",
        "**1. Curse of Dimensionality:**\n",
        "- High-D data: Sparse, far apart, overfitting risk.\n",
        "- Solution: Reduce dimensions first.\n",
        "\n",
        "**2. Approaches:**\n",
        "- Projection: Linear (PCA).\n",
        "- Manifold: Nonlinear (LLE).\n",
        "\n",
        "**3. PCA:**\n",
        "- Preserve variance.\n",
        "- Variants for large/streaming data.\n",
        "- Compression (lossy).\n",
        "\n",
        "**4. kPCA:**\n",
        "- Nonlinear PCA via kernels.\n",
        "- Tune with GridSearch.\n",
        "\n",
        "**5. LLE:**\n",
        "- Unrolls manifolds.\n",
        "- Good for twisted data.\n",
        "\n",
        "**6. Pro/Con Reduction:**\n",
        "- **Pro:** Faster training, less memory, sometimes better perf.\n",
        "- **Con:** Lossy, complex pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Exercises (from the book)\n",
        "\n",
        "### Exercise 1\n",
        "**Q:** What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
        "**A:**\n",
        "**Motivasi Utama:**\n",
        "- Speed up training.\n",
        "- Save memory/storage.\n",
        "- Visualize data (2D/3D).\n",
        "- Improve performance (remove noise/useless features).\n",
        "\n",
        "**Kelemahan:**\n",
        "- Kehilangan informasi (lossy).\n",
        "- Dapat menurunkan performa model.\n",
        "- Membuat pipeline model menjadi lebih kompleks dan lebih sulit dirawat.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2\n",
        "**Q:** What is the curse of dimensionality?\n",
        "**A:** Ini adalah fakta bahwa banyak hal berperilaku sangat berbeda di ruang berdimensi tinggi. Data menjadi sangat **jarang (sparse)**; instance pelatihan cenderung sangat **jauh satu sama lain**. Ini membuat prediksi menjadi tidak dapat diandalkan (berdasarkan ekstrapolasi besar) dan meningkatkan risiko **overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3\n",
        "**Q:** Once a dataset's dimensionality has been reduced, is it possible to reverse the operation?\n",
        "**A:** **Tergantung.**\n",
        "* **PCA:** Ya, dapat dibalikkan menggunakan metode `inverse_transform()`. Namun, ini adalah \"rekonstruksi\" yang **lossy** (ada kehilangan data); Anda tidak mendapatkan data asli kembali karena Anda membuang varians.\n",
        "* **kPCA:** Ya, tetapi lebih rumit. Ini disebut menemukan \"pre-image\". Anda dapat mengaktifkannya dengan `fit_inverse_transform=True`.\n",
        "* **LLE:** (Teks tidak menyebutkan metode inversi langsung).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 4\n",
        "**Q:** Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
        "**A:** **Ya, bisa,** tapi PCA adalah algoritme linier yang akan mencoba \"meratakan\" data. Ini akan efektif dalam mengurangi dimensi tetapi akan kehilangan semua struktur nonlinier yang kompleks (misalnya, \"menghancurkan\" Swiss roll menjadi satu bidang datar). Untuk dataset nonlinier, **kPCA** atau **LLE** adalah pilihan yang jauh lebih baik.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5\n",
        "**Q:** Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
        "**A:** **Tidak mungkin diketahui** tanpa melihat datasetnya. Jumlah dimensi akan bergantung pada struktur internal data tersebut. Jika datasetnya MNIST (dari 784D), hasilnya adalah 154D. Jika dataset lain, bisa jadi 100D, atau 500D. Jawabannya **bergantung pada dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6\n",
        "**Q:** In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?\n",
        "**A:**\n",
        "* **Vanilla PCA (Full SVD):** Pilihan default untuk dataset yang muat di memori.\n",
        "* **Incremental PCA (IPCA):** Ketika dataset **terlalu besar** untuk muat di memori (out-of-core) atau untuk **data streaming (online)**.\n",
        "* **Randomized PCA:** Ketika $d$ (dimensi target) jauh lebih kecil dari $n$ (fitur asli) dan Anda membutuhkan **performa yang jauh lebih cepat**.\n",
        "* **Kernel PCA (kPCA):** Ketika dataset **nonlinier** (misalnya, terpilin seperti Swiss roll).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 7\n",
        "**Q:** How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
        "**A:**\n",
        "1.  **Kualitas Pipeline (Supervised):** Ukur performa dari *tugas downstream* (misalnya, klasifikasi). Reduksi dimensi yang baik adalah yang meningkatkan akurasi atau sangat mempercepat pelatihan tanpa terlalu merusak akurasi akhir.\n",
        "2.  **Reconstruction Error (Unsupervised):** Ukur \"reconstruction error\" (atau \"pre-image error\" untuk kPCA). Reduksi dimensi yang baik memiliki error rekonstruksi yang rendah.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8\n",
        "**Q:** Does it make any sense to chain two different dimensionality reduction algorithms?\n",
        "**A:** **Ya.** (Teks tidak secara eksplisit membahas ini, tetapi ini adalah praktik umum). Misalnya, Anda dapat menggunakan PCA untuk menghilangkan redundansi linier dengan cepat, lalu menggunakan t-SNE atau LLE pada data yang sudah dikurangi tersebut untuk memvisualisasikan struktur manifold nonliniernya.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 9 & 10\n",
        "**Q:** (MNIST Exercises)\n",
        "**A:** (Requires implementation)\n",
        "* **Ex. 9:** Latih Random Forest pada MNIST. Lalu gunakan PCA (95% variance). Latih Random Forest lagi pada data tereduksi. Bandingkan waktu dan akurasi.\n",
        "* **Ex. 10:** Gunakan t-SNE untuk mengurangi MNIST menjadi 2D dan plot hasilnya. Bandingkan visualisasi dengan PCA, LLE, atau MDS.\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Reducing! üìâ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}